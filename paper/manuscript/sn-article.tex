%%\documentclass[referee,sn-basic]{sn-jnl}% referee option is meant for double line spacing
%%\documentclass[lineno,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style

\documentclass[default,pdflatex,iicol]{sn-jnl}% Default with double column layout

%%<additional latex packages if required can be included here>
\usepackage[numbers]{natbib}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{makecell}
\usepackage{placeins}
\usepackage{amssymb}

\jyear{2021}
\raggedbottom
%%\unnumbered% uncomment this for unnumbered level heads

\begin{document}

\title[Article Title]{Rubber Tapping Line Detection in Near-Range Images via Multibranch Convolutional Neural Network}

\author[]{\fnm{Rattachai} \sur{Wongtanawijit}}\email{rattachai.gov@gmail.com}
\equalcont{These authors contributed equally to this work.}

\author*[*]{\fnm{Thanate} \sur{Khaorapapong}}\email{thanate.khao@gmail.com}
\equalcont{These authors contributed equally to this work.}

\affil*[1]{\orgdiv{Department of Computer Engineering, Faculty of Engineering}, \orgname{Prince of Songkla University}, \orgaddress{\street{P.O. Box 2 Kohong}, \city{HatYai, Songkhla}, \postcode{90112}, \country{Thailand}}}

\abstract{Current image-task-related convolutional neural network structures lean toward the directed acyclic graph that allows multiple output nodes. This enables a solution for the rubber tapping line detection that various desired output types, such as bounding boxes, points in pixels or edges, are necessary. This paper demonstates a multibranch deep convolutional networks with their outputs are bounding boxes and pixel segmentation masks by adopting YOLOv3 and U-Net structures. This paper proposes the functions of column-wise argmax and column-wise Softmax with the redundant mask outputs intended to enhance pixel classification accuracy. Experiments with the networks discover that some novel segmentation loss functions, such as Dice's cofficient, Focal, Tversky's index, have different characters for the tapping line prediction which were observed by Hausdorff distance and F1-score. The network with multiple mask predictions can omit their weaknesses and yield higher tapping line detection accuracy compare to every single ones. Also, the column-wise Softmax and argmax well suit to the tapping line detection in place of a thinning algorithm in the image processing context.}

\keywords{\textit{hevea brasiliensis}, Semantic Edge, Rubber Tapping, Tapping Line, YOLO}

\maketitle

%% Section
\section{Introduction}\label{sec-introduction}

A self-instructed robot that has abilities to detect things and interact with, a sensory device with a detection algorithm is at least required. The detail of a vision system of a rubber tapping robot using a robotic arm, e.g. \cite{patent1, patent2, patent3, WANG2022103906, Zhou2021rubber}, which introduced in \cite{Wongtanawijit_2021}, adopted a near-range camera co-operating with a dedicated algorithm to perform tapping line detection. Continuing on the previous work, this paper presents another tapping line detection in a different way. Given the definition of a tapping line in an image, is a piecewise polynomial where its vertices are at the pixels in the image grid \cite{Wongtanawijit_2021}. Then, the detection is to precisely locate that pixels.

Motivations of this paper arise from deep convolutional neural networks such as object bounding-box detectors, mask, edge-aware or semantic edge segmentation networks. Nevertheless, none of them are the instant solution for this problem. Original contributors of those works designed their networks to fit into more generic data than the tapping line images. This encourages us to explore other researches that answer how those generic networks can be adopted for some specific datasets that might be useful for the tapping line detection.

Considering the essences of CNN-based object detectors architectures, many of them shared some common properties such as layer configurations. For example, the downstream convolution path shrinks down the input's spatial dimensions. In any arbitary layer orders, high-level detection which known as "classification" happens in low-resolution tensors, while other lower semantics such as edge detection are performed at higher tensor's resolution. This is because individual parts in CNN play the different roles. Some layers extract the useful information, while others shape that information to the desire outputs which specified by the loss functions. Notable differences are at the network's output or head. A box detector's output is the grid coordinates which represent the location and the dimension of the boxes, while the output of segmentation mask is the probability score at each pixels.

For the tapping line, this paper demonstates a deep convolutional neural network that composed of multiple output types. Box outputs roughly determine where the tapping line is in a rectangular area. Pixel classification output answers where the polyline's vertices are inside the box. Network's structure are arranged in a directed acyclic computational graph in order to make the network trainable with gradient back-propagation optimizer with the end-to-end training scheme. A pixel classification head obtains the data from the downsampling-upsampling path likewise in U-Net \cite{unet}, V-Net \cite{vnet} or other pixel-level segmentation networks. Co-operating with the proposed column-wise argmax function, the networks can directly produce tapping lines without any post-processing. The network calculates the boxes' coordinates at two prediction heads with the legacy YOLO \cite{redmonyolov2} 's regression form.

Training to detect tapping lines, a set of novel loss for pixel classfication on very unbalanced data, Dice's coefficient, Binary Focal, and Tversky indices are inspected for loss-value convergence and the prediction accuracy. The overall loss for the network is the combination of YOLO's box regression and pixel classification losses. Assessing the network's performance, a set of evaluation metrics are applied to the predictions. The Intersection-Over-Union (IoU) ratios of the box clearify the existence of a tapping line in an image. F1-score and distance errors describe the closeness of the predicted pixels to the actual tapping line inside the box.

Detection results appear to be diverse when the network are trained with different pixel classification loss. There are some trade-off between F1-score and Hausdorff distance, but all is on par with each others. Every models provide above 93.0 percents in average precision with the bounding box evaluation. The results also point that the appending of column-wise Softmax to the pixel prediction head helps the networks to gain more accuracy than the widely used pixel-wise activated neurons. 

In order to archieve higher detection accuracies, this paper also presents a modification in high level layers of the pixel classification branch for separately predict multiple loss or deep supervising, then applying the aggregation by voting for the pixel majority with the use of column-wise Softmax. The proposed method can generate the results in higher F1-scores with also diminishes the distance between two polylines.

The rests of this paper include: the review of literatures in Section \ref{sec-review} \textemdash consists of the previous work of the tapping line detection, relevant applications of muli-output CNN detectors, loss function and its concern etc. The design of tapping line detection network with evaluation methods are in Section \ref{sec-methodology}. The experiment setup and results are discussed in Section \ref{sec-exp} and Section \ref{sec-results}. Conclusion is summarized in Section \ref{sec-conclude}.

\section{Background and Motivation}\label{sec-background}
The purpose of the near-range vision camera is for precisely locate the tapping position for tapping manipulation of the robot arm. Our previous work \cite{Wongtanawijit_2021} demonstrated the detection algorithm in a form of an image processing algorithm. The method produces accurate results but it was specially designed and required two input images accommodating by artificial lighting patterns for the nighttime farming. Apart from this paper that, the CNN-based detectors can learn what and where is the tapping line under any lighting circumstance depends on the training dataset. Moreover, tappling line detection by modern computer vision algorithms is not yet explored at this time. Hence, it is a interesting topic to study the capable of CNN-based detectors with the tappline line image dataset, and might be useful as an alternative algorithm for the robot.

%% Section
\section{Review of Literatures}\label{sec-review}
\subsection{Tapping Line Detection}
Rubber tapping is a process of farming the natural rubber latex from a ParÃ¡ rubber tree (\textit{Hevea brasiliensis}). Tapping is making a cut on the rubber tree bark or the tapping panel letting the latex to flow along. This cut is leaving a gutter that called the tapping path. Refer to the rubber tapping document \cite{abraham1992tapping}, the cut is recommended to be 30 to 45 degrees angle to the ground axis. Likewise, small angle tolerances are acceptable. A robot with a robotic arm integrated, which can do tapping automatically, probably have been installed a close-up camera for fine detail acquisition as in many visual-servoing systems (patents \cite{patent1, patent2, patent3}). Consequently, the appearance of a tapping path is a certain curve in the image which named the "tapping line".

The image data with labels are necessary for developing a machine learning algorithm by the supervised learning. The previous work \cite{Wongtanawijit_2021} labels a tapping line by two annotations. As shown in Fig \ref{fig-sampleannotation}, a bounding box encodes the tapping endpoints that to be at the top-left and the bottom-right corner of the box. Then, a polyline or a piecewise polynomial connect to its corners as the tapping line endpoints. As a results, the polyline's vertices can be defined on every columns inside the box for the annotation convenience and satisfying the spiral downward tapping system.

\begin{figure}[h]%
\centering
\includegraphics[width=\linewidth]{sample_annotation.pdf}
\caption{A near-range tapping panel image (left) and the magnified cropped (right). The tapping line is annotated by a bounding box (dotted green rectangle) and a polyline (multiple connected red segments along the box's diagonal).}\label{fig-sampleannotation}
\end{figure}

\subsection{Overview of Deep Convolutional Neural Networks}
Cutting-edge ideas on deep CNN-based detectors quite becomes settled in recent years. Back then when CNN-based box detectors, Faster-RCNN \cite{fasterrcnn} and YOLO \cite{redmonyolov1}, were established in 2015. Then the much more recent detectors such as YOLOv3 \cite{redmonyolov3} (2018), YOLOv4 \cite{alexyyolov4} (2020) able to detect any objects in the images with such very high rates than their predecessors. Later researches put lots of improvements on the modern networks. For some details, YOLOv3 and v4 have multiple prediction head that works on different spatial resolution so that the detector better locates small objects in previous YOLO and even better than the Faster-RCNN. In the later version, YOLOv4, exploits the path skipping in various stages with dense and sparse prediction layers. This make YOLOv4 push the network produce the better results. But, the mechanism of bounding box's coordinate prediction in YOLOv3 and YOLOv4 still the same. Nonetheless, the aim of the improvements is to make the network to learn or to discover the features of the data as much as possible. Since the datasets that used in those detector development can be seen as a generic dataset \textemdash a dataset that an image may contain various numbers of object instances and object classes. 

Apart from boxes, there are other networks that can predict examining the image in pixel level as well. Some novel networks such as U-Net \cite{unet}, V-Net \cite{vnet}, SegNet \cite{segnet}, etc., produce a mask from the image as the segmentation category in image processing. These models separate the areas of the objects (or other interests) from the background. These models employ a chain of upsampling or transposed convolution layers to enlarge the spatial size of the data back to input's size, which are suitable for fine pixel detection application.

Another example is the semantic-aware prediction. The networks not only mark the object into the pixel area but also recognize which classes of each pixels in the mask are associate with. Remarkable examples are CASENet \cite{casenet}, Mask-RCNN \cite{maskrcnn}, etc. However, these networks are build on the purpose of the generic datasets as mentioned earlier. Lots of network architecture are proposed and indicated the powerful of convolutional networks on the computer vision tasks. It remains a challenge to adopt those networks addressing some applications in specific domains.

\subsection{Network Loss Function}\label{review_loss}
Mapping of values between responses (values returned from a network) with expectations (target values) is the core principle of any neural network. The difference  between targets and responses, which calculated by a function, is a loss, and the such function is named ``loss function". Many researches on CNN-based detectors present loss functions that refined for their particular problems. For instances, Dice's coefficient index \cite{vnet, gendice}, well robustly handle a segmentation task in the medical imaging. Focal loss \cite{focalloss1}, an extension of cross-entropy function, is appropriate for a class-imbalanced data. Tversky index \cite{tverskyloss} also reduces the effects of unbalanced data to be learned by the segmentation network.

By understanding the nature of the data that they are working on especially in the segmentation problems, many researches did some mathematical adjustments on the exist loss functions, letting their networks produce preferable results. Focal loss \cite{focalloss1} reformulates binary classic binary cross-entropy function by add the extra weight on $\gamma$ term to address the ``hard example" not only foreground-background imbalance, since they foreseen their dataset have included some problematic samples. As well as the increment of $\beta$ term of Tversky index \cite{tverskyloss} preventing the network to over-learn by the false-positive samples (improve recall), because their interested points are very sparse in amount and space compared to the background. So, selecting a loss function for a network in a particular problem is crucial for network's performance not only the layer configuration of the network.

Meanwhile, the losses of bounding box's coordinates regression in all of the YOLOs and R-CNN variants, stay on L-norm functions. Discussion in the performance of L2-norm and L1-norm indicates that the regression with L1 is more robust than L2 in loss dynamics. And L2 tends to cause more numerical instability issue than L1 does, so that it should used with a small learning rate \cite{fastrcnn}.

\subsection{Deep Learning Computer Vision in Agricultural Robotics}
Conducting the powerful deep CNN-based computer vision to agricultural robotics becomes popular in following years after mentioned algorithms are established. Lots of them focused on ripe fruit detections in robotized harvesting by adapting detection algorithms to facilitate their objectives. For instance in tomato detections with YOLO, Guoxu Liu et al. \cite{s20072145} changed the bounding box output of YOLOv3 to the ``circular bounding box" with a radius regression equation which agrees with a circular shape of tomatoes, while \cite{Lawal_2021, chentomato} stay on rectangular bounding boxes and add new layer configurations to achieve higher detection accuracy than the standard YOLO Darknet-53. There are also a lot of examples for this category to be reviewed later in the following section such as, apple detection and segmentation with U-Net \cite{Li2021apple}, Mask-RCNN \cite{Chu2021apple, Jia2020apple}, YOLOv3 \cite{Kuznetsova2020apple}, Mask-RCNN on strawberry \cite{Yu2019strawberry}, etc.

Inspecting for details on the mentioned works that how they did the modifications on standard detector networks, a popular mod is to add multi-scale predictions to the network (a.k.a feature-pyramid \cite{Tong2020}) in order to enhance the recognition of object in various sizes, especially for discover small objects. This technique add dedicate paths in the network graph, which are found in such as \cite{chentomato, Lawal_2021, Yu2019strawberry, Li2021apple}. Researches of such as \cite{Jia2020apple, Chu2021apple, Yu2019strawberry, Lawal2021tomato} employ the adventages of feature re-use that presented in densely-convolution backbone \cite{densenet} or the residual-block of ResNet \cite{resnet}, which efficiently increase the network learning capability. Mish activation function is more prefered over ReLU family in order to improve network training dynamics and final accuracy, but this might comes with more computational expense \cite{misra2019mish}.

In segmentation applications, downsampling upsampling path of U-Net already included the feature-pyramid structure. \cite{Li2021apple} tackles bluring problem of input images by add gated-convolution between each spatial resolution changes in U-Net. Suppression Mask-RCNN \cite{Chu2021apple} appends another convolutional network with an additional window sliding operation to the end of Mask-RCNN for filtering unwanted features which causes the overall network becomes a longer chain and requires a separate training scheme.

All of those mentioned works decided to stick with standard loss functions from original papers, multi-class or binary cross entropy losses for classification, L1-smooth for RCNN's box, and averge of pixelwise cross-entropy for segmentation loss. They rarely show the use of other losses, e.g. loss that designed for imbalanced data, as mentioned in section \ref{review_loss}. This is because labels of their datasets rather be in the balanced shape so they can ignore this regard.

\subsection{Computer Vision for Rubber Latex Harvesting}
Maneuvering a robot approaches a rubber tree in a near-range, likewise in \cite{zhang2019rubber, WANG2022103906}, with aiming a onboarded camera to recoginze the position of a tapping path in the tapping panel. The tapping line's appearance in the image can be considering as an edge or a curve line that trace along the trunk curvature. Thus, detection of a tapping line falls into a category of semantic edge detection \textemdash detect edges that be parts of objects. This unlikes the detection of the solid objects or the closed-contour shapes in the fruit picking sences. For example, training the model for tapping line detection might be facing more data imbalanced issues because edges' pixels are much less than the backgound. A bounding box that placed onto a tapping line encodes tapping endpoint information and a tapping line edge is a polygonal line, as decribed in \cite{Wongtanawijit_2021}. Therefore, a tapping line detector might include a box detector with a segmentation branch. The implementations have high tendency to use YOLO over R-CNN variants due to efficiency, low computational cost which is feasible for a mobile platform.


%% Section
\section{Methodology}\label{sec-methodology}

\subsection{Model Design}\label{subsec-model}
Fig \ref{fig1-modelA} illustrates the overview of the tapping line detection network in a block diagram. The computational graph consists of a downsampling backbone network for extracting features, two box detection heads at different spatial resolutions for object size coverages, a upsampling path that enlarges tensors' size back to the input's size, and a pixel classification head for the detection of tapping line's pixels. This one-staged design facilitates the end-to-end training. Since, tapping line detection is a one-class object detection problem and likely to be deployed on a mobile robot. Hence, a small or compact network, in term of numbers of weights and data's sizes, is favourable. As a result, the following model construction has 5.53 million trainable parameters in approximate.

\begin{figure*}[!ht]%
\centering
\includegraphics[width=\textwidth]{fig1.pdf}
\caption{The proposed tapping line detection network, visualized in a directed grpah with blocks. Network composes of four parts, from left-to-right are Downsampling network, two YOLO detection branches, Upsampling network, and Pixel classification branch.}\label{fig1-modelA}
\end{figure*}

\subsubsection{Downsampling Network}
The downsampling network borrows residual block structures of Darknet-53. The modifications are trimmed out the numbers of filters, shrunk the overall network depth and reduced the input size to $224 \times 224 \times 3$\footnote[1]{tensor's size naming convention given by width $\times$ height $\times$ depth, and also applied across this paper}, while preserving filter's sizes and LeakyReLU activation function, as shown in Table \ref{tab-darknetlight}. This network path is shared as a feature extractor backbone among two YOLO detection branchs, the upsampling path, and the pixel classification branch.

\begin{table}[!h]
\centering
\caption{Downsampling backbone network explained in block structures with given names (\textbf{block}). The denoted labels are layer operation (ops), a 2-dimension convolution (Conv2d), a shortcut connection with addition (Residual), $s$: convolution stride, $n$: numbers of filters, filter kernel's size (\textbf{filter}), and $c$ is the numbers of block replication in a serial chain.}
\label{tab-darknetlight}
\resizebox{\linewidth}{!}{%
\begin{tabular}{lcccccc}
\toprule
\textbf{block}      & $c$ 			  & ops 		& $n$   		& \textbf{filter} 	& $s$ 	& \textbf{output} 	\\ \midrule
Input                 &                    & -                       			&    		&       				&  	& $224\times224\times3$   \\ \midrule
                      	&                    & Conv2d                  		& 32 		& $3\times3$   	& 1 	& $224\times224\times32$  \\
down1               	&                    & Conv2d                  		& 64  	& $3\times3$    	& 2 	& $112\times112\times64$  \\ \midrule
\multirow{3}{*}{res1} & \multirow{3}{*}{1} & Conv2d            & 32  	& $1\times1$    	& 1 	&             	\\
                      	&                    & Conv2d                  		& 64  	& $3\times3$    	& 1 	&   \\
                      	&                    & Residual             			&     		&        			  	&   	& $112\times112\times64$  \\ \midrule
down2             	&                    & Conv2d                  		& 128 	& $3\times3$    	& 2 	& $56\times56\times128$   \\ \midrule
\multirow{3}{*}{res2} & \multirow{3}{*}{2} & Conv2d            & 64  	& $1\times1$    	& 1 	&      \\
                      	&                    & Conv2d                  		& 128 	& $3\times3$    	& 1 	&   	\\
                      	&                    & Residual             			&     		&        			  	&   	&	$56\times56\times128$   \\ \midrule
down3               	&                    & Conv2d                  		& 256 	& $3\times3$    	& 2 	&	$28\times28\times256$   \\ \midrule
\multirow{3}{*}{res3} & \multirow{3}{*}{4} & Conv2d            & 128 	& $1\times1$    	& 1 	&      \\
                      	&                    & Conv2d                  		& 256 	& $3\times3$    	& 1 	&    	\\
                      	&                    & Residual             			&     		&        				&   	& $28\times28\times256$   \\ \midrule
down4                &                    & Conv2d                  		& 384 	& $3\times3$    	& 2 	& $14\times14\times384$   \\ \midrule
\multirow{3}{*}{res4} & \multirow{3}{*}{4} & Conv2d            & 192 	& $1\times1$    	& 1 	&      \\
                      	&                    & Conv2d                  		& 384 	& $3\times3$    	& 1 	&    	\\
                      	&                    & Residual             			&     		&        				&   	& $14\times14\times384$   \\ \midrule
down5               	&                    & Conv2d                  		& 512 	& $3\times3$    	& 2 	& $7\times7\times512$     \\ \midrule
\multirow{3}{*}{res5} & \multirow{3}{*}{1} & Conv2d            & 256 	& $1\times1$    	& 1 	&      \\
                      	&                    & Conv2d                  		& 512 	& $3\times3$    	& 1 	&      \\
                      	&                    & Residual             			&     		&        				&   	& $7\times7\times512$     \\ \bottomrule
\end{tabular}%
}
\end{table}

\subsubsection{Box Detection Branches}
Box detection heads comply with YOLO's box prediction method. Two YOLO box detection branches connect to the downsampling network at the output of block ``res4" and ``res5", referred to Table \ref{tab-darknetlight}. Each branch has a private $3\times3$ convolution with 512 filters which useful for learning the spatial-resolution-specific feature at the particular feature map's size ($w \times w$, in Table \ref{tab-yolohead}). Then, their outputs connect to every ends which remaps tensor's depth to the number of anchors by pointwise convolutions for producing the final box predicted values. There are five box output ends, where fours are box coordinates ($t_x, t_y, t_w, t_h$) and one is for the objectness scores, detailed in Table \ref{tab-yolohead}. Three anchors are applied, where anchors' size are calculated on mean of samples of the dataset, and the rests are fixed at $\pm15\%$ aspect ratio distortions on the mean value.

According to Table \ref{tab-yolohead}, hyperbolic tangent function ($\mathrm{tanh}$) activates the offset values of anchors' centers. Sigmoid is applied for objectness scores. Anchors' dimension offsets are set to un-activated outputs.

\begin{table}[]
\centering
\caption{Structure of a YOLO detection branch with the three prior boxes or anchors, where $n$ represents the number of filters, and activation function is denoted by $f(x)$. $w$ and $d$ orderly represents input tensor's spatial size and its depth, which may come from ``res4" or ``res5".}
\label{tab-yolohead}
\begin{minipage}{\linewidth}
\begin{tabular}{lccccc}
\toprule
\textbf{block} 		& ops & $n$ 	& \textbf{filter} 	& $f(x)$ & \textbf{output} 			\\ \midrule
\multicolumn{4}{l}{\makecell[l]{res4 ($w=14, d=384$),\\ res5 ($w=7, d=512$)}} 	&	& $w \times w\times d$      \\ \midrule
private  & Conv2d   & 512			& $3\times3$          & leaky           	& $w \times w\times512$      \\ \midrule
\multicolumn{6}{c}{Split}                                                                            \\ \midrule
$t_x$		& Conv2d       & 3          & $1\times1$             & tanh              & $w \times w \times 3$         \\
$t_y$		& Conv2d       & 3          & $1\times1$             & tanh              & $w \times w \times 3$         \\
$t_w$		& Conv2d       & 3          & $1\times1$             & -                   & $w \times w \times 3$         \\
$t_h$		& Conv2d       & 3          & $1\times1$             & -                   & $w \times w \times 3$         \\
obj		& Conv2d       & 3          & $1\times1$             & $\sigma$        & $w \times w \times 3$         \\ \bottomrule
\end{tabular}
\end{minipage}
\end{table}

\subsubsection{Upsampling Network}
Network's upsampling path employs the U-Net structure that low resoultion tensors are enlarged by transpose convolutions whose inputs are taken from same previous layers, shown in Fig \ref{fig1-modelA}. Following the U-Net convention, tensors in every spatial changes in backbone layers are gathered, by depth-concatenation operation, with one from previous transpose convolution for the next transpose convolution step. Activation functions for all layers of upsampling path are LeakyReLU as well as in the backbone network. Explaination in details is in Table \ref{tab-unetpath}.

\begin{table}[]
\centering
\caption{Configuration of U-Net-like Upsampling Path, where ops represent a transpose convolution path in Fig \ref{fig1-modelA}, and $n$ is the number of filters}
\label{tab-unetpath}
\begin{minipage}{\linewidth}
\begin{center}
\begin{tabular}{p{0.15\linewidth}p{0.15\linewidth}p{0.5\linewidth}}%>%{@{}lcr@{}}
\toprule
ops 	& $n$ 	& \textbf{input}  					\\ \midrule
u1	&128	    & $7 \times 7 \times 512$              	        \\
u2	&128	    & $14 \times 14 \times (384+128)$    	     \\ 
u3	&64	    & $28 \times 28 \times (256+128)$    	       \\
u4	&64	    & $56 \times 56 \times (128+64)$      	        \\
u5	&32	    & $112 \times 112 \times (64+64)$    	        \\ 
\bottomrule
\end{tabular}%
\end{center}
\footnotetext{Note: All 2D transpose convolution are done with kernel's size $2\times2$ and stride 2, causing the enlarging factor of 2.}
\end{minipage}
\end{table}

\subsubsection{Pixel Classification Branch}
Network's pixel classification section proposes three pixel prediction blocks or the Pxl Headers for the writing convenience, named ``Pxl Header1", ``Pxl Header2", and ``Pxl Header3" in Fig \ref{fig1-modelA}. Table \ref{tab-pxlheader} outlines the function layers on each Pxl Header, where all block has the same compoments. A tensor which size $224 \times 224 \times 64$, that comes from the last layer of the upsampling network, is simultaneously fed into every pixel prediction blocks. Table \ref{tab-pxlheader} presents the structure of Pxl Header. The results from every pixel prediction blocks are fed to the mapping function $\mathrm{g}(x)$ that produces tapping line pixels in a binary image.

\begin{table}[]
\centering
\caption{Components inside a pixel prediction block (Pxl Header), the executions of ops perform in the order}
\label{tab-pxlheader}
\begin{minipage}{\linewidth}
\begin{center}
\begin{tabular}{@{}clcc@{}}
\toprule
Order 	& ops 	& \textbf{input} $d$\footnotemark[1]  & \textbf{output} $d$\footnotemark[1]	\\ \midrule
1	& Pointwise Conv2d	& 64  & 1 \\
2	& Sigmoid Activation	& 1  & 1 \\
3	& Column-wise Softmax   & 1 & 1 \\ \bottomrule
\end{tabular}%
\end{center}
\footnotetext[1]{All tensor's spatial size equals to $224\times 224 \times d$}
\end{minipage}
\end{table}

Two-dimension convolution with filter size of $1\times1$ (also known as pointwise convolution) with filter's depth of 64 transforms the size of input tensor to $224\times 224\times 1$. Then let the Sigmoid activation modifies the values. Pxl Header introduces the Column-wise Softmax layers. According to the tapping line definition that only one pixel in a column contains a tapping line's pixel, so applying a standard unit Softmax function on each image's column conforms such annotations. Sum-to-one property of Softmax adds the row-position correlation within the image's columns, which means it suppressed others while boosting one another. This process can be a reminiscence to an edge-thinning algorithm in image processing context, and also helps the model's training to be more likely converged faster. 

For inferencing a tapping line, function $\mathrm{g}(x)$ composed of three operations, that run in the order by,
\begin{enumerate}
\item finds the pixel locations which have the highest probability on each column (columnwise-argmax) of each input.
\item checks for the majorities, 2 of 3, on pixel location from step 1.'s results. (Voting Operation)
\item binarizes the majority locations (set one for majority pixels, otherwise set to zeros)
\end{enumerate}

In the network traning phase, pixel classification losses are computed on each output of pixel prediction blocks, which means function $\mathrm{g}(x)$ and beyonds are not contribute for the loss calculation. Learnable weights in a particular Pxl Header combine low-level of high resolution tensor from early-stage layers and high-level features extracted from down-sampling to up-sampling layers via any mapping function, which can be arbitary specified in network traning phase. Sigmoid function bounds the values in between 0 and 1 representing the valid pixl existence probability, which also controls the numerical stability on the neurons and enhances the training convergence.

\subsection{Model Training}
Dataset for this paper contains 1,425 tapping line images, only RGB1 version \cite{Wongtanawijit_2021} is used. Complying K-fold cross validation scheme with k=5, image data are divided into five clusters (folds). In each fold, there are 285 samples for evaluation and 1,140 samples for training the model.

\subsubsection{Dataset Priors and Statistics}
All images in the dataset is collected within the nighttimes using artificial light souces. The camera produces tapping panel images with the dark background. And, an tapping panel image contains only one tapping line.

All RGB image are resized to network's input size, $224\times224\times3$ pixels with not preserving their original aspect ratios. An averaged size of annotated tapping line bounding boxes, with respected to the resized image, is 59.2 width and 70.6 height. Therefore, the values estimates for anchors' sizes are (59.2, 70.6), (50.2, 79.6), and (66.6, 60.0). These anchors will be constantly used across all models.

\begin{figure}[h]%
\centering
\includegraphics[width=0.25\linewidth]{fig2.pdf}
\caption{Visualization of the estimated anchors. All box's aspect ratios are in scale. Dashed line represents the averaged box}\label{fig2-anchors}
\end{figure}

Estimation of anchors' size also implies the unbalancing between tapping line's pixels and the background pixels. The annotation regulation of the tapping line (single pixel per column) gives the approximation ratio, by the anchor's width to the box's area : $59.2 / 224^2 \approx 0.0012$, which indicates highly imbalanced data between two classes.

\subsubsection{Branch Loss Function}\label{subsubsec-pxlbranch}
A benifit of one-stage network is that the calculation of losses happens at the output nodes. According to the model, output nodes are two YOLO heads, and three pixel prediction blocks (Pxl Headers). Thus, the total loss is the linear combination of them.

MSE loss (L2-loss) is used for box's coordinates regression on both YOLO detection heads, and Dice's coefficient scores the objectness predictions on the feature-map grids. For the pixel classification loss, calculation on each pixel predicton block is handled by different loss functions. Dice's coefficient, Binary Focal, and Tversky index with an emphasize on $\beta$ are separately used on each Pxl Header, on account of highly imbalance between tapping line's pixels and background's pixels. Since, adding more weight on $\beta$ in Tversky's formular \cite{tverskyloss} will increase the loss when tapping line pixels are predicted as background, which are the false-negative case.

\subsubsection{Parameters Settings}\label{subsubsec-trainingparams}
Training of the model on every folds shares same training parameters. Mini-batch gradient descent with momentum optimizer is used with mini-batch size of 4, and momentum coefficient constantly at 0.9. Training for a model processes in a range of 90 to 100 epochs, since the early-stopping may be applied in a particular model. Learning rates are scheduled by the WarmUp-Steady-Decay profile with a peak of 0.008, shown in Fig \ref{fig3-LR}. All the weights on the model are newly initialized. 

Unit-rescaling and z-score transformation normalize the pixel values of an input image across all its depth dimension (color channel). Batch normalization is also appiled inside the layers that contain learnable weights for the training purpose.

\begin{figure}[h]%
\centering
\includegraphics[width=0.66\linewidth]{fig3.pdf}
\caption{WarmUp-Steady-Decay Learning Rate Profile. Learning rate grows and drops in exponential rates in the warm-up and the decay periods.}\label{fig3-LR}
\end{figure}

\subsubsection{Generalization}\label{subsubsec-generalization}
Training the model adopts a couple of tools, such as weight regularization, image augmentation, weight drop-out, etc., to prevent the model from overfitting. L2 regularization\footnote[2]{Concerning the stabillity of training dynamics with single precision floating point, the regularization factor is set to $2/m$ where $m$ is the number of learnable weights} is used on the learnable weights in all network's layers. For the data generalization, the image processings like the random rotation within $\pm20^\circ$ and the magnification within $\pm20\%$ are applied on the input images for the augmentation purpose. Another tool is checking the box predictions of YOLO outputs. If the box predictions reach 0.75 IoU or highers with the groundtruth, then the box's loss calculations will omit in that paricular spatial location on YOLO attention maps.

\subsection{Model Inferencing}
Box detection branches and the pixel classification branch separately produce the outputs. Pixel classification end predicts a binary image that the valid pixels (1's) may be outside the predicted box from YOLO branches. To connect them together with agrees to the dataset that an image has only one tapping line, the network is going to produce one bounding box as well.

For producing bounding box prediction, the network selects only one strongest prediction for an anchor size per YOLO branches, which means there are 3 predictions at a YOLO branch. Six predictions from 2 YOLO branches are therefore reduced to one final result by Non-maximal suppression (NMS).

At the pixel classication branch, function $\mathrm{g}(x)$ creates a binary image that contains one valid pixel (1's) per column. Some valid pixels may aligned with the tapping line while some are not. So, the prediction is to crop the binary image by the final bounding box from NMS to filter out the unwanted pixels. Boundary condition in box cropping is dominated by the box, valid pixels on the box's boderline are removed.

Accomplishing the meaningful result from both annotations, the final polyline will have the first vertex at box's top-left corner as the tapping line starting point, and the terminate vertex at box's bottom-right corner for the tapping line terminating point. Fig \ref{fig-sampleagreement} illustrates an example of the process. This agreement is for inferencing a tapping line from the model.
% sample_tapline_box_agreement
\begin{figure}[h]%
\centering
\includegraphics[width=0.52\linewidth]{sample_tapline_box_agreement.pdf}
\caption{A drawing shows a polyline and a box that are locating in a discrete unit grid. Assuming that a intersection on the grid represents a pixel location in an image. The red box represents the NMS-processed box with a plolyline (top). The result that the polyline are cropped and re-assigned end vectices to the box's corners is in bottom.
}\label{fig-sampleagreement}
\end{figure}

\subsection{Taping Line Evaluation Method}
The bounding box can justify the existence of a tapping line in an image, using the boxes overlapping ratios between predictions and priors such as Jaccard Index (a.k.a IoU), Dice's coefficient, etc. Box overlapping ratios nonetheless do not reflect the similarity of the tapping line \cite{Wongtanawijit_2021}. Pixel-level overlapping ratios or geometry distances can measure for similarities or distance errors between two polygonal curves that encoded in image's pixels.

The bounding box overlapping ratios with 0.5 and 0.75 IoU levels determine average precisions of the detections. And, pixel-level Dice's coefficient (a.k.a F1-score) on the tapping line's pixels verifies the exact overlap between two polygonal's vertices. However, appling the overlapping measurement on the polygonal vertices in the pixel-level over penalizes the small mislocated pixels. As visualized in Fig \ref{fig4-polyline}, the predicted polyline has a small shift in distance to the reference line. This makes the Dice's index approaching to zero, whereas a 2-D geometric distance, Hausdorff distance, indicates strong similarity between them.

\begin{figure}[h]%
\centering
\includegraphics[width=0.4\linewidth]{fig4.pdf}
\caption{A drawing of two polylines in a discrete unit grid. Assuming that a intersection on the grid represents a pixel location in an image. A detected tapping line (dashed line) merely distances to the groundtruth (solid line) with 1 pixel is the exact overlapped.}\label{fig4-polyline}
\end{figure}

To clearify the calculation of both metrics, let any polygonal lines where thier vertices are on a discrete grid (positive integers grid, $\mathbb{I^+}\times \mathbb{I^+}$), which also known as the ``discrete curve". So, let $\textbf{A}$ and $\textbf{B}$ respectively represent the set of vertices of a predicted tapping line and its corresponding groundtruth. Dice's coefficient or F1-score ($D$) is obtained in equation \ref{eq-dice}
\begin{equation}
D = \frac{2 \lvert \textbf{A} \cap \textbf{B} \rvert }{\lvert\textbf{A}\rvert+\lvert\textbf{B}\rvert}\label{eq-dice}
\end{equation}
Hausdorff distance ($\mathrm{d_H}$), measures the similarity between two curves, is defined by equation \ref{eq-hausdorff} \cite{Huttenlocher1993ComparingIU}, where $a \in \textbf{A}$ and $b \in \textbf{B}$. Function $\mathrm{d}(\textbf{A},\textbf{B})$ denotes for the directed Hausdorff distance (one-way) from curve \textbf{A} to curve \textbf{B}.
\begin{align}
\mathrm{d}(\textbf{A},\textbf{B}) &= \max_{a\in\textbf{A}} \min_{b\in\textbf{B}} \| a-b \|  \nonumber \\
\mathrm{d_H}(\textbf{A},\textbf{B}) &= \max (\mathrm{d}(\textbf{A},\textbf{B}), \mathrm{d}(\textbf{B},\textbf{A})) \label{eq-hausdorff}
\end{align}

In addition to the mentioned box and polyline evaluation metrics, this paper also includes the L2-distance ($\mathrm{d_{ep}}$) of tapping endpoints where two tapping endpoints locate at the top-left and bottom-right corners of the box. This measurement adds more details on the box's location over IoU ratios. Let a bounding box is in a form of [x, y, width, height]. Denote that the predicted box is $[x_p, y_p, w_p, h_p]$ and the groundtruth box is $[x_g, y_g, w_g, h_g]$. The predicted tapping endpoints are $ep_{p1} = (x_p, y_p)$ and $ep_{p2} = (x_p+w_p, y_p+h_p)$. Likewise, the groundtruth tapping endpoints are $ep_{g1} = (x_g, y_g)$ and $ep_{g2} = (x_g+w_g, y_g+h_g)$. So, the average endpoint distance is defined by equation \ref{eq-endpoint}.
\begin{equation}
\mathrm{d_{ep}} = \frac{1}{2} (\| ep_{p1}- ep_{g1} \| + \| ep_{p2}  - ep_{g2} \|) \label{eq-endpoint}
\end{equation}

Annotation with a box and a polyline enables the evaluations in many combinations. This paper presents a method by using the bounding boxes to decide for the existance of a tapping line in the image, then calculates other metrics within the detected results.

\section{Experiment Setup}\label{sec-exp}
The design of the model in section \ref{subsec-model} follows YOLOv3 and U-Net architectures, whereas the proposed modifications are made at output branches on the assumption that the modfications contribute to the model's prediction accuracy. Briefly, the proposed pixel classification branch, Fig \ref{fig5-proposedPxlHeader}, employs three parallel blocks (Pxl Header) for mapping between prediction and data by three different segmentation loss functions designed for highly imbalanced data, Dice's, Focal and Tversky's index, with the auxiliary column-wise Softmax. Lastly, function $\mathrm{g}(x)$ attains incoming data from those blocks, producing the final binary mask. These components are critical for producing tapping line's pixels.

\begin{figure}[h]%
\centering
\includegraphics[width=0.55\linewidth]{fig5.pdf}
\caption{Pixel classification branch of the proposed model}\label{fig5-proposedPxlHeader}
\end{figure}

The proposed model's components can be summarized as.
\begin{itemize}
  \item Column-wise Softmax in Pxl Header.
  \item Parallel Pxl Headers with function $\mathrm{g}(x)$.
  \item Multiple pixel classification loss for training the model.
\end{itemize}

Those assumptions require reference models or the baseline models in order to notice the components' contributions. The baseline model construction is piecewise shuffling those components in the layer chain, such as reducing the amount of Pxl Header, bypassing the column-wise Softmax, etc. As a result, the experiment gives the labels of the models by,
\begin{itemize} 
\item[] \emph{\textbf{Model A}}: The proposed model, as presented in Section \ref{subsec-model}. \\
\item[] \emph{\textbf{Model B}}: Similar to Model A but column-wise Softmax functions are removed from all Pxl Headers. \\
\item[] \emph{\textbf{Model C}}: Made changes on Model A by,
	\begin{itemize}
  	\item Remove the parallel structure of three Pxl Headers, only one single block is used as shown in Fig \ref{fig6-baselinePxlHeader}. Pxl Header's structure is similar to one of the proposed model. 
  	\item Only one segmentation loss is used for training the pixel classification output at a time.
  	\item Re-define $\mathrm{g}(x)$ by removing the voting operation from the original (in Section \ref{subsubsec-pxlbranch}), because there is only one incoming signal from Pxl Header.
	\end{itemize}
\item[] \emph{\textbf{Model D}}: Similar to Model C, but column-wise Softmax layer is no longer in the Pxl Header.
\end{itemize}

The structure of pixel classication branch of Model C and Model D is shown in Fig \ref{fig6-baselinePxlHeader}. Eventually, the training parameters of all models is the same settings as described in Section \ref{subsubsec-trainingparams}

\begin{figure}[h]%
\centering
\includegraphics[width=0.55\linewidth]{fig6.pdf}
\caption{Pixel classification branch of baseline models (Model C and Model D)}\label{fig6-baselinePxlHeader}
\end{figure}

\section{Results and Discussions}\label{sec-results}
Fig \ref{fig-sampledetection} presents some detection results that the original images are overlayed by boxes and polylines of groundtruths and predictions. 

\begin{figure*}[]%
\centering
\includegraphics[width=\linewidth]{sample_detection.pdf}
\caption{Some detection samples from various models. A pair of two images presents a tapping panel image with the original size to scale (left) beside its magnified view (right). The predicted boxes with polyline insides are displayed in yellow. The dashed box with a polyline in red is the groundtruth}\label{fig-sampledetection}
\end{figure*}

The following numeric results in this section are averaged from the models that test with K-fold cross-validaton scheme. Table \ref{tab-results-ap50} and Table \ref{tab-results-ap75} present average precisions of tapping line bounding box detection using 0.5 IoU ($\mathrm{AP}_{50}$) and using 0.75 IoU ($\mathrm{AP}_{75}$) alongside distance errors of tapping endpoints, F1-scores, and discrete Hausdorff distances that are averaged from the box-detected samples. Note that the calculation of the discrete Hausdorff distance manipulates the distance between polylines by the distances between thier pixel vertices. 

\begin{table*}[!ht]
\centering
\caption{Detection results on the proposed model and baseline models at various settings. All metrics are averaged on the box-detected samples at 0.5 IoU}
\label{tab-results-ap50}
\begin{minipage}{\linewidth}
\begin{center}
\begin{tabular}{clccccc}
\toprule
Model & Pxl Header loss & \makecell[c]{Col-wise Softmax} & $\mathrm{AP_{50}}$ &$\mathrm{d_{ep}}$(px.) & $\mathrm{d_{H}}$(px.)& F1-score	\\ \midrule
A 	& Dice, Focal, Tversky 	& \checkmark	& 0.9697	&	12.65		& 	9.56		&	0.4226	\\ \midrule
B 	& Dice, Focal, Tversky 	& -				& 0.9822	&	12.13		& 	9.16		&	0.5738	\\ \midrule
C	& Dice						& \checkmark	& 0.9544	&	13.50		&	10.80		& 	0.3836	\\
C	& Focal						& \checkmark 	& 0.9794	&	12.49		&	13.20		&	0.3727	\\ 
C	& Tversky					& \checkmark	& 0.9475	&	15.46		&  13.71		&	0.3964   \\ \midrule
D 	& Dice						& - 				& 0.9359	&	14.69		&	13.20		&	0.5454	\\
D 	& Focal						& - 				& 0.9516	&	14.27		&	13.24		&	0.4081	\\
D 	& Tversky					& - 				& 0.9679	&	12.06		&	21.28		&	0.5666	\\ 
\bottomrule
\end{tabular}%
\end{center}
%\footnotetext{Note: Model code A, B, C refer to naming in Section \ref{sec-exp}}
\end{minipage}
\end{table*}

\begin{table*}[!h]
\centering
\caption{Detection results on the proposed model and baseline models at various settings. All metrics are averaged on the box-detected samples at 0.75 IoU}
\label{tab-results-ap75}
\begin{minipage}{\linewidth}
\begin{center}
\begin{tabular}{clccccc}
\toprule
Model & Pxl Header loss & \makecell[c]{Col-wise Softmax} & $\mathrm{AP_{75}}$ &$\mathrm{d_{ep}}$(px.) & $\mathrm{d_{H}}$(px.)& F1-score	\\ \midrule
A 	& Dice, Focal, Tversky 	& \checkmark	& 0.5801	&	9.32		& 	7.20		&	0.4277	\\ \midrule
B 	& Dice, Focal, Tversky 	& -				& 0.6157	&	9.19		& 	6.99		&	0.5915	\\ \midrule
C	& Dice						& \checkmark	& 0.5160	&	9.71		&	7.73		& 	0.4009	\\
C	& Focal						& \checkmark 	& 0.5829	&	9.32		&	11.07		&	0.3884	\\ 
C	& Tversky					& \checkmark	& 0.4043	&	10.57		&  9.87		&	0.4219   \\ \midrule
D 	& Dice						& - 				& 0.4520	&	10.00		&	9.68		&	0.5664	\\
D 	& Focal						& - 				& 0.4698	&	9.76		&	10.09		&	0.4293	\\
D 	& Tversky					& - 				& 0.6059	&	9.22		&	19.60		&	0.5876	\\ 
\bottomrule
\end{tabular}%
\end{center}
%\footnotetext{Note: }
\end{minipage}
\end{table*}

\subsection{Box Detection}
Results in Table \ref{tab-results-ap50} and Table \ref{tab-results-ap75} clearly show that all models can detect tapping line boxes with high average precisions of 0.95 and aboves for 0.5 box IoU ($\mathrm{AP_{50}}$). Even in the more precise localization criterion, results with 0.75 box IoU ($\mathrm{AP_{75}}$) also confirm that all models also consistently produce tapping line bounding box at similar rates about 0.4 to 0.6 average precision. These IoU-based results correlate to tapping endpoints errors that measure the distance between boxes' corners. 
\subsection{Pixel Detection}
Results in Fig \ref{fig-sampledetection}, all model produces closely results on the visual. The fact that there are some differences in pixel level that can be expressed by discrete Hausdorff distance and F1-scores in the result tables. Among proposed model variants, Model B achieves highest averaged F1-scores, 0.57$@$0.5 box IoU and 0.59$@$0.75 box IoU, with small averaged discrete Hausdorff distances, 9.16 pixels$@$0.5 box IoU and 6.99$@$0.75 box IoU within the $224 \times 224$ image resolution. These values indicate the predicted tapping line is in a very close proximity and the strong similarity to the groundtruth.

The key operation of tapping line's pixel prediction is in the function $\mathrm{g}(x)$. Checking on every column where is the highest probability pixel location (mathematical equivalence to the argmax for the row position on every column in an image) agrees to the tapping line pixel-level annotation and it is helpful to produce the final pixel classification output in a binary mask. 

\subsection{Effects of Column-wise Softmax on Tapping Line's Vectices Detection}
Results from single Pxl Header networks, Model C and Model D, show the effects of column-wise Softmax in the pixel classification. Comparing two models with one Pxl Header that using the same pixel classificaion loss, the model with no columnwise Softmax produce tapping lines with higher F1-scores but also slightly raise Hausdorff distances. Inversely, models with column-wise Softmax can predict tapping lines with lower Hausdorff distances but slightly decline the F1-scores. This is clearly noticeable in Table \ref{tab-results-ap50} and Table \ref{tab-results-ap75}, in the cases of Model C and Model D that using Dice's cofficient and Tversky's index as their pixel classification loss function.

Eventually, this effect is invalid in the models with three Pxl Headers (Model A and Model B), because the output data from Pxl Heaers are further processed by $\mathrm{g}(x)$ with voting operation unlike the direct output of the single Pxl Header networks.

What does the tapping line detection that result in a high Hausdorff distance with also high F1-scores especially on Model D imply? An example in Fig \ref{fig-sampleHausdorffDice} displays the actual results on a test image that come from Model C and Model D trained by the Focal pixel clasification loss. Model C gives a tapping line with $\mathrm{d_{H}}=4.47$ and $D=0.5203$. And, the tapping line from Model D has $\mathrm{d_{H}}=48.7$ and $D=0.5230$. The example informs the posibility that the predicted tapping line with high F1-scores and also high Hausdorff distance may have small fraction of sorely mis-positioned pixels, while the pixels in large fraction are aligned to the groundtruth. The uses of column-wise Softmax could cope this issue but also results in sacrifying the F1-score, which is the distinguishable difference between Model C and Model D. However, by the tapping line inference using box cropping, Hausdorff distance and F1-scores cannot catch the irregular predicted pixels that are outside the predicted box, which the final result may yield a small Hausdorff distance by the pixels inside the box.

\begin{figure}[!h]%
\centering
\includegraphics[width=\linewidth]{sampleHausdorffDice.pdf}
\caption{Detection samples from Model C (top) and Model D (bottom) from same test image. The predicted polyline is in the blue box, and the polyline within the red dashed box is the groundtruth. The magnified views in the right side are for better visualization. The upper predicted tapping line has $\mathrm{d_{H}}=4.47$ and $D=0.5203$, and the lower predicted tapping line has $\mathrm{d_{H}}=48.7$ and $D=0.5230$.}\label{fig-sampleHausdorffDice}
\end{figure}

\subsection{Multiple Pixel Prediction Heads with activation function $\mathrm{g}(x)$}
Adding the pixel prediction reslience to the model by parallel multiple pixel prediction branches as in Model A and Model B, the redundant top-level learnable layers are freely mapped their weights to the satisfy values that specified by a particular loss function. To be precise, with the gradient back-propagating path, this technique allows the network to learn some common features that are shared among pixel classification branches (Pxl Header). And, the learnable layers in pixel classification branches learn the loss-specific features to predict the tapping line's pixels. This technique also applied in two YOLO detection branches which they have thier own private weights.

To train and construct Model A and Model B, that uses three different loss on three Pxl Headers and with and without using the columnwise Softmax, chronologically comes from the basis of results in Model C and Model D. Depicting the Model C, Dice's coefficient and Tversky loss noticeably produce results with high F1-scores while the results from Dice, Focal and Tversky loss are in par by the Hausdorff distance metric. Besides the basis that the column-wise Softmax can enchance the tapping line results in Hausdorff distances. 

Aggregation for the final output from three binary mask that come from three Pxl Headers, function $\mathrm{g}(x)$ on Model A and Model B demonstates a voting strategy. Two of three pixel coincidence are expected from three Pxl Headers with an assumption that voting can clear out outlier pixel that cause large Hausdorff distances while the coincident in pixel is the verification. Results in Table \ref{tab-results-ap50} and Table \ref{tab-results-ap75} confirm the assumption. Model A and Model B outperform all single pixel classification branch (Pxl Header) variants in both Hausdorff distance and F1-score. Nonetheless, tapping line pixel's accuracy on Model A that using column-wise Softmax on Pxl Headers lacks behind the non-column-wise Softmax Model B by a significant margin.

Moreover, expanding the number of Pxl Header in accord to the model design could make the aggregation function on $\mathrm{g}(x)$ not trivial. Network designers have to examine the behaviour of thier model on thier dataset and responsible for the loss function.

\subsection{Network Computation Time}
Table \ref{tab-computetime} presents the average times in seconds that are spended for the network training and inferencing by each model. Although Model A and Model B have included more trainable layers in Pxl Headers than the Model C or Model D does, but there is a small margin in the training time and there is no significant gain in the inference time. However, the measurement is done on a desktop computer with GPU acceleration enabled\footnote[3]{Intel 4.40 GHz hexa-core CPU with Nvidia GTX 1070Ti} and network implementations are based on the Julia with Flux.jl \cite{innes2018} ecosystem.

\begin{table}[]
\centering
\caption{Averaged Computation Times by Model}
\label{tab-computetime}
\begin{minipage}{\linewidth}
\begin{center}
\begin{tabular}{ccc}
\toprule
Model & \makecell[c]{Avg. Training Time\footnotemark[1] \\(sec/epoch)}  & \makecell[c]{Avg. Inference Time\footnotemark[2]\\(sec/img)}	\\ \midrule
A	& 99.0	& 0.28  \\
B	& 98.7	& 0.28  \\
C	& 95.5   & 0.27 \\ 
D	& 94.5   & 0.27  \\ \bottomrule
\end{tabular}%
\end{center}
\footnotetext[1]{Note that the training time have already included L2-regularization, image loading and resizing, and the runtime image augmention as in the section \ref{subsubsec-generalization}}
\footnotetext[2]{Tapping line inference time also includes image loading, resizing to match the network input's size, and image normalization}
\end{minipage}
\end{table}

\section{Conclusion}\label{sec-conclude}
To summarize the contributions, this paper have established the deep convolutional neural object detection networks for the tappine line detection in near-range images. The network adepted the YOLO bounding box prediciton method with the down-sampling to up-sampling path as usual in many segmentation networks, particularly for the U-Net architecture in the design. 

The design adds multiple learnable layers in the top level of the network for boxes and pixel predictions. Based on the tapping line's vertices annotation, the operations like column-wise Softmax and argmax for the pixel position are examined for the pixel prediciton. The experiments with multibranch pixel prediction proved that they have the ability to empower the pixel prediction accuracy through the proposed operation with the voting on aggregation which supports the training by multiple loss functions, while rarely causes a small computation overhead compare to the network with single pixel prediction header. The implementation details of all network layers are also provided. 

The paper offers a tapping line inferencing method using bounding boxes and polylines that are defined by pixels. The analysis for the unwanted artifacts on a predicted tapping line by two evaluation metrics, Hausdorff distance and F1-score, are presented. Results show that the proposed network design archieves the highest prediction rate at 98.2 percents on box accuracy with 9.16 Hausdorff distance error in pixels or 2.89 percent to the image's diagonal ($2.89=9.16/(224\sqrt2)\times100\%$).

Additionally for the further development, we will aim at the 3D tapping path construction which is based on this paper works with the use of depth information.

\backmatter

\bmhead{Acknowledgments}
Giving acknowledgements to Graduated School, Prince of Songkla University for research funding and to Computer Engineering Department at Prince of Songkla University for computing infrastructure. Especially for the rubber latex farmers, Mr.Jan and Mrs.Ree, in rubber trees provision. Unfortunately for Mrs.Ree that she's experiencing the amnesia from her accident which made us sympathize and wish special thanks to her.

\section*{Declarations}
\begin{itemize}
\item Availability of Near-range Tapping Line Dataset: please send an inquiry to the coresponding author Mr.Thanate Khaorapapong
\item Source code is made avaiable at \url{https://github.com/rwttr/NearcamTaplineDetector}, all implementation is done on Julia Language with Flux.jl machine learning library \cite{innes2018}.
\end{itemize}

\bibliographystyle{bst/sn-basic}
\bibliography{sn-bibliography}
\end{document}