%%\documentclass[referee,sn-basic]{sn-jnl}% referee option is meant for double line spacing
%%\documentclass[lineno,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style

\documentclass[default,pdflatex,iicol]{sn-jnl}% Default with double column layout

%%<additional latex packages if required can be included here>
\usepackage[numbers]{natbib}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{makecell}
\usepackage{tabularx}

\jyear{2021}
\raggedbottom
%%\unnumbered% uncomment this for unnumbered level heads

\begin{document}

\title[Article Title]{Nearcam Tapline Detection 2}

\author[]{\fnm{Rattachai} \sur{Wongtanawijit}}\email{rattachai.gov@gmail.com}
\equalcont{These authors contributed equally to this work.}

\author*[*]{\fnm{Thanate} \sur{Khaorapapong}}\email{thanate.khao@gmail.com}
\equalcont{These authors contributed equally to this work.}

\affil*[1]{\orgdiv{Department of Computer Engineering, Faculty of Engineering}, \orgname{Prince of Songkla University}, \orgaddress{\street{P.O. Box 2 Kohong}, \city{HatYai, Songkhla}, \postcode{90112}, \country{Thailand}}}

\abstract{Current image-task-related convolutional neural network structures lean toward the directed acyclic graph that allows multiple output nodes. This enables a solution for the rubber tapping line detection that various desired output types, such as bounding boxes, pixel points or edges, are necessary. This paper demonstates that such deep networks with their outputs are bounding box and semantic pixel segmenation mask by adopting YOLO and U-Net layer structures. This paper proposes a column-wise argmax function with the technique of redundant mask outputs intended to enhance pixel classification accuracy. Experiments with the networks discover that some novel segmentation loss functions have different characters for tapping line prediction which were observed with geometric distance and F1-score. The redundant mask output can omits their weaknesses and yield higher detection accuracy compare to every single ones. Also, the column-wise argmax well suit to the tapping line detection in place of a thinning algorithm rather than the thresholding on the spatial output neurons.}

\keywords{\textit{hevea brasiliensis}, Semantic Edge, Rubber Tapping, Tapping Line, YOLO}

\maketitle

%% Section
\section{Introduction}\label{sec-introduction}

A self-instructed robot that has abilities to detect things and interact with, a sensory device with a detection algorithm is at least required. The detail of a vision system of a rubber tapping robot using a robotic arm, e.g. \cite{patent1, patent2, patent3, WANG2022103906, Zhou2021rubber}, which introduced in \cite{Wongtanawijit_2021}, adopted a near-range camera co-operating with specifically designed algorithm to perform tapping line detection. Continuing on the previous work, this paper presents another tapping line detection algoirthm in a whole different way. Given the definition of a tapping line in an image, is a piecewise polynomial where its vertices are at the pixels in the image grid \cite{Wongtanawijit_2021}. The detection is to precisely locate that pixels.

Motivations of this paper come from recent deep convolutional neural nets (CNN), such as object bounding-box detector, mask, edge-aware or semantic edge segmentation networks. But, none of them are ready-to-use solution for this problem. Original contributors of those works designed their networks to fit into more generic data than the tapping line images. This encourages this paper to explore other researches that answer how can those generic networks be adopted for some more specific datasets that might be useful for the tapping line detection.

Considering the essences of CNN-based object detectors architectures, many of them shared some common properties such as layer configurations. For example, the downstream convolution path shrinks down the input's spatial dimensions. In any arbitary layer orders, high-level detection which known as "classification" happens in low-resolution tensors, while other lower semantics such as edge detection are performed at higher tensor's resolution. This is because individual parts in CNN play the different roles. Some layers extract the useful information, while others shape that information to the desire outputs which specified by the loss functions. Notable differences are at the network's output or head. A box detector's output is the grid coordinates which represent the location and the dimension of the boxes, while the output of segmentation mask is the probability score at each pixels.

For the tapping line, this paper demonstates a network that composed of multiple different output types. Box outputs roughly determine where the tapping line is in a rectangular area. Pixel classification output answers where the polyline's vertices are wihtin the box. Network's structure are arranged in a directed acyclic computational graph in order to make the network trainable with gradient-based back-propagation optimizer with the end-to-end training scheme. A pixel classification head obtains the data from the downsampling-upsampling path likewise in U-Net \cite{unet}, V-Net \cite{vnet} or other pixel-level segmentation networks. Co-operating with the proposed column-wise argmax function, the networks can directly produce tapping lines without any post-processings. The network calculates the boxes' coordinates at two prediction heads with the earlier YOLO \cite{redmonyolov2} 's regression form.

Training to detect tapping lines, a set of novel loss for pixel classfication on very unbalanced data, Dice-Coefficient, Binary Focal, and Tversky indices are inspected for loss-value convergence and the prediction accuracy. The overall loss for the network is the combination of YOLO's box regression and pixel classification losses. Assessing the network's performance, a set of evaluation metrics are applied to the predictions. The Intersection-Over-Union (IoU) ratios of the box clearify the existence of a tapping line in an image. F1-score and distance errors describe the closeness of the predicted pixels to the actual tapping line inside the box.

Detection results appear to be diverse when the network are trained with different pixel classification loss. There are some trade-off between F1-score and Hausdorff distance, but all is on par with each others. Every models provide above 92.0 percents in average precisions with bounding box evaluation. The results also point that by the appending of column-wise Softmax to the pixel prediction head helps the networks to gain more accuracy than the widely used pixel-wise activated neurons. 

In order to archieve the higher detection accuracy, this paper also presents a modification in top layers of pixel classification head for separately predict multiple loss or deep supervising, then gather all of them by voting for the pixel majorities with the use of column-wise Softmax function. The proposed method can generate the results in higher F1-score with also diminishes the distance between two polylines.

The rests of this paper include: the review of literatures in Section \ref{sec-review} \textemdash e.g. the previous work of the tapping line detection, relevant applications of muliple output node CNN detectors, etc. The design of tapping line detection network in Section 3. The experiment setup and results in Section 4 and Section 5. Summary with the disscussions are in Section 6.

\section{Background and Motivation}\label{sec-background}
The purpose of the near-range vision camera is for precisely locate the tapping position for tapping manipulation of the robot arm. Our previous work \cite{Wongtanawijit_2021} demonstrated the detection algorithm in a form of an image processing algorithm. The method produces accurate results but it was specially designed and required two input images accommodating by artificial lighting patterns for the nighttime farming. Apart from this paper that, the CNN-based detectors can learn what and where is the tapping line under any lighting circumstance depends on the training dataset. Moreover, tappling line detection by modern computer vision algorithms is not yet explored at this time. Hence, it is a interesting topic to study the capable of CNN-based detectors with the tappline line image dataset, and might be useful as an alternate algorithm for the robot.

%% Section
\section{Review of Literatures}\label{sec-review}
\subsection{Tapping Line Detection}
Rubber tapping is a process of farming the natural rubber latex from a ParÃ¡ rubber tree (\textit{Hevea brasiliensis}). Tapping is making a cut on the rubber tree bark or the tapping panel letting the latex to flow along. This cut is leaving a gutter that called the tapping path. Refer to the rubber tapping document \cite{abraham1992tapping}, the cut is recommended to be 30 to 45 degrees angle to the ground axis. Likewise, small angle tolerances are acceptable. A robot with a robot arm integrated, which able to do tapping automatically, probably have been installed a close-up camera for fine detail acquisition as in many visual-servoing systems (patents \cite{patent1, patent2, patent3}). Consequently, the appearance of a tapping path is a certain line in the image which named the "tapping line".

The image data with labels are necessary for developing a machine learning algorithm by the supervised learning. Previous tapping line detection work \cite{Wongtanawijit_2021} labels a tapping line by two annotations. A bounding box encodes the tapping endpoints that to be at the top-left and the bottom-right corner of the box. Then, a polyline or a piecewise polynomial connect two endpoints. As a results, the polyline's vertices can be defined on every columns inside the box in order to simplify the annotations.

\subsection{Overview of Deep Convolutional Neural Networks}
The developments of core ideas on deep CNN-based detectors quite become settled in last 5 years. Back then when CNN-based box detectors, Faster-RCNN \cite{fasterrcnn} and YOLO \cite{redmonyolov1}, were established in 2015. Much more recent detectors such as YOLOv3 \cite{redmonyolov3} (2018), YOLOv4 \cite{alexyyolov4} (2020) able to detect any objects in the images with such very high rates than their predecessors. Later researches put lots of improvements on the modern networks. For some details, YOLOv3 and v4 have multiple prediction head that works on different spatial resolution so that the detector better locates small objects in previous YOLO and even better than the Faster-RCNN. In the later version, YOLOv4, exploits the path skipping in various stages with dense and sparse prediction layers. This make YOLOv4 push the network produce the better results. But, the mechanism of bounding box's coordinate prediction in YOLOv3 and YOLOv4 still the same. Nonetheless, the aim of the improvements is to make the network to learn or to discover the features of the data as much as possible. Since the datasets that used in those detector development can be seen as a generic dataset \textemdash a dataset that an image may contain various numbers of object instances and object classes. 

Apart from boxes, there are other networks that can predict examining the image in pixel level as well. Some novel networks such as U-Net \cite{unet}, V-Net \cite{vnet}, SegNet \cite{segnet}, etc., produce a mask from the image as the segmentation task in image processing category. These models separate the areas of the objects (or other interests) from the background. These models employ a chain of upsampling or transposed convolution layers to enlarge the spatial size of the data back to input's size, which are suitable for fine pixel detection application.

Another example is the semantic-aware prediction. The networks not only mark the object into the pixel area but also recognize which classes of each pixels in the mask are associate with. Remarkable examples are CASENet \cite{casenet}, Mask-RCNN \cite{maskrcnn}, etc. However, these networks are build on the purpose of the generic datasets as mentioned earlier. Lots of network architecture are proposed and indicated the powerful of convolutional networks on the computer vision tasks. It remains a challenge to adopt those networks addressing some applications in specific domains.

\subsection{Network Loss Function}\label{review_loss}
Mapping of values between responses (values returned from a network) with expectations (target values) is the core principle of any neural network. The difference  between targets and responses, which calculated by a function, is a loss, and the such function is named ``loss function". Many researches on CNN-based detectors present loss functions that refined for their particular problems. For instances, Dice-Coefficient index \cite{vnet, gendice}, well robustly handle a segmentation task in the medical imaging. Focal loss \cite{focalloss1}, an extension of cross-entropy function, is appropriate for a class-imbalanced data. Tversky index \cite{tverskyloss} also reduces the effects of unbalanced data to be learned by the segmentation network.

By understanding the nature of the data that they are working on especially in the segmentation problems, many researches did some mathematical adjustments on the exist loss functions, letting their networks produce preferable results. Focal loss \cite{focalloss1} reformulates binary classic binary cross-entropy function by add the extra weight on $\gamma$ term to address the ``hard example" not only foreground-background imbalance, since they foreseen their dataset have included some problematic samples. As well as the increment of $\beta$ term of Tversky index \cite{tverskyloss} preventing the network to over-learn by the false-positive samples (improve recall), because their interested points are very sparse in amount and space compared to the background. So, selecting a loss function for a network in a particular problem is crucial for network's performance not only the layer configuration of the network.

Meanwhile, the losses of bounding box's coordinates regression in all of the YOLOs and R-CNN variants, stay on L-norm functions. Discussion in the performance of L2-norm and L1-norm indicates that the regression with L1 is more robust than L2 in loss dynamics. And L2 tends to cause more numerical instability issue than L1 does, so that it should used with a small learning rate \cite{fastrcnn}.

\subsection{Deep Learning Computer Vision in Agricultural Robotics}
Conducting the powerful deep CNN-based computer vision to agricultural robotics becomes popular in following years after mentioned algorithms are established. Lots of them focused on ripe fruit detections in robotized harvesting by adapting detection algorithms to facilitate their objectives. For instance in tomato detections with YOLO, Guoxu Liu et al. \cite{s20072145} changed the bounding box output of YOLOv3 to the ``circular bounding box" with a radius regression equation which agrees with a circular shape of tomatoes, while \cite{Lawal_2021, chentomato} stay on rectangular bounding boxes and add new layer configurations to achieve higher detection accuracy than the standard YOLO Darknet-53. There are also a lot of examples for this category to be reviewed later in the following section such as, apple detection and segmentation with U-Net \cite{Li2021apple}, Mask-RCNN \cite{Chu2021apple, Jia2020apple}, YOLOv3 \cite{Kuznetsova2020apple}, Mask-RCNN on strawberry \cite{Yu2019strawberry}, etc.

Inspecting for details on the mentioned works that how they did the modifications on standard detector networks, a popular mod is to add multi-scale predictions to the network (a.k.a feature-pyramid \cite{Tong2020}) in order to enhance the recognition of object in various sizes, especially for discover small objects. This technique add dedicate paths in the network graph, which are found in such as \cite{chentomato, Lawal_2021, Yu2019strawberry, Li2021apple}. Researches of such as \cite{Jia2020apple, Chu2021apple, Yu2019strawberry, Lawal2021tomato} employ the adventages of feature re-use that presented in densely-convolution backbone \cite{densenet} or the residual-block of ResNet \cite{resnet}, which efficiently increase the network learning capability. Mish activation function is more prefered over ReLU family in order to improve network training dynamics and final accuracy, but this might comes with more computational expense \cite{misra2019mish}.

In segmentation applications, downsampling upsampling path of U-Net already included the feature-pyramid structure. \cite{Li2021apple} tackles bluring problem of input images by add gated-convolution between each spatial resolution changes in U-Net. Suppression Mask-RCNN \cite{Chu2021apple} appends another convolutional network with an additional window sliding operation to the end of Mask-RCNN for filtering unwanted features which causes the overall network becomes a longer chain and requires a separate training scheme.

All of those mentioned works decided to stick with standard loss functions from original papers, multi-class or binary cross entropy losses for classification, L1-smooth for RCNN's box, and averge of pixelwise cross-entropy for segmentation loss. They rarely show the use of other losses, e.g. loss that designed for imbalanced data, as mentioned in section \ref{review_loss}. This is because labels of their datasets rather be in the balanced shape so they can ignore this regard.

\subsection{Computer Vision for Rubber Latex Harvesting}
Maneuvering a robot approaches a rubber tree in a near-range, likewise in \cite{zhang2019rubber, WANG2022103906}, with aiming a onboarded camera to recoginze the position of a tapping path in the tapping panel. The tapping line's appearance in the image can be considering as an edge or a curve line that trace along the trunk curvature. Thus, detection of a tapping line falls into a category of semantic edge detection \textemdash detect edges that be a part of an object. This unlikes from detecting a solid object or a closed-contour shape in the fruit picking sences. For example, training the model for tapping line detection might be facing more data imbalanced issues because edges' pixels are much less than the backgound. A bounding box that placed onto a tapping line encodes tapping endpoint information and a tapping line edge is a polygonal line, as decribed in \cite{Wongtanawijit_2021}. Therefore, a tapping line detector might include a box detector with a segmentation branch. The implementations have high tendency to use YOLO over R-CNN variants due to efficiency, low computational cost which is feasible for a mobile platform.


%% Section
\section{Methodology}\label{sec-methodology}


\subsection{Models}\label{subsec-model}
Fig \ref{fig1-modelA} illustrates the overview of the tapping line detection network in a graph of block diagram. The computational graph consists of a downsampling backbone network for extracting features, two box detection heads at different spatial resolutions for object size coverages, a upsampling path that enlarges tensors' size back to the input's size, and a pixel classification head for the detection of tapping line's pixels. This one-staged design facilitates the end-to-end training. Since, tapping line detection is a one-class object detection problem and likely to be deployed on a mobile robot. Hence, a small or compact network, in term of numbers of weights and data's sizes, is favourable.

\begin{figure*}[h]%
\centering
\includegraphics[width=\textwidth]{fig1.pdf}
\caption{The proposed tapping line detection network, visualized in a directed grpah with blocks. Network composes of four parts, from left-to-right are Downsampling network, two YOLO detection branches, Upsampling network, and Pixel classification branch.}\label{fig1-modelA}
\end{figure*}

\subsubsection{Downsampling Network}
The downsampling network borrows residual block structures of Darknet-53. The modifications are trimmed out the numbers of filters, shrunk the overall network depth and reduced the input size to $224 \times 224 \times 3$\footnote[1]{tensor's size naming convension given by width $\times$ height $\times$ depth, and also applied across this paper}, while preserving filter's sizes and LeakyReLU activation function, as shown in Table \ref{tab-darknetlight}. This network path is shared as a feature extractor backbone among two YOLO detection branchs, the upsampling path, and the pixel classification branch.

\begin{table}[]
\centering
\caption{Downsampling backbone network explained in block structures with given names (\textbf{block}). The denoted labels are layer operation (ops), a 2-dimension convolution (Conv2d), a shortcut connection with addition (Residual), $s$: convolution stride, $n$: numbers of filters, filter kernel's size (\textbf{filter}), and $c$ is the numbers of block replication.}
\label{tab-darknetlight}
\resizebox{\linewidth}{!}{%
\begin{tabular}{@{}lcccccc@{}}
\textbf{block}      & $c$ 			  & ops 		& $n$   		& \textbf{filter} 	& $s$ 	& \textbf{output} 	\\ \midrule
Input                 &                    & -                       			&    		&       				&  	& $224\times224\times3$   \\ \midrule
                      	&                    & Conv2d                  		& 32 		& $3\times3$   	& 1 	& $224\times224\times32$  \\
down1               	&                    & Conv2d                  		& 64  	& $3\times3$    	& 2 	& $112\times112\times64$  \\ \midrule
\multirow{3}{*}{res1} & \multirow{3}{*}{1} & Conv2d            & 32  	& $1\times1$    	& 1 	&             	\\
                      	&                    & Conv2d                  		& 64  	& $3\times3$    	& 1 	&   \\
                      	&                    & Residual             			&     		&        			  	&   	& $112\times112\times64$  \\ \midrule
down2             	&                    & Conv2d                  		& 128 	& $3\times3$    	& 2 	& $56\times56\times128$   \\ \midrule
\multirow{3}{*}{res2} & \multirow{3}{*}{2} & Conv2d            & 64  	& $1\times1$    	& 1 	&      \\
                      	&                    & Conv2d                  		& 128 	& $3\times3$    	& 1 	&   	\\
                      	&                    & Residual             			&     		&        			  	&   	&	$56\times56\times128$   \\ \midrule
down3               	&                    & Conv2d                  		& 256 	& $3\times3$    	& 2 	&	$28\times28\times256$   \\ \midrule
\multirow{3}{*}{res3} & \multirow{3}{*}{4} & Conv2d            & 128 	& $1\times1$    	& 1 	&      \\
                      	&                    & Conv2d                  		& 256 	& $3\times3$    	& 1 	&    	\\
                      	&                    & Residual             			&     		&        				&   	& $28\times28\times256$   \\ \midrule
down4                &                    & Conv2d                  		& 384 	& $3\times3$    	& 2 	& $14\times14\times384$   \\ \midrule
\multirow{3}{*}{res4} & \multirow{3}{*}{4} & Conv2d            & 192 	& $1\times1$    	& 1 	&      \\
                      	&                    & Conv2d                  		& 384 	& $3\times3$    	& 1 	&    	\\
                      	&                    & Residual             			&     		&        				&   	& $14\times14\times384$   \\ \midrule
down5               	&                    & Conv2d                  		& 512 	& $3\times3$    	& 2 	& $7\times7\times512$     \\ \midrule
\multirow{3}{*}{res5} & \multirow{3}{*}{1} & Conv2d            & 256 	& $1\times1$    	& 1 	&      \\
                      	&                    & Conv2d                  		& 512 	& $3\times3$    	& 1 	&      \\
                      	&                    & Residual             			&     		&        				&   	& $7\times7\times512$     \\ \bottomrule
\end{tabular}%
}
\end{table}

\subsubsection{Box Detection Branches}
Box detection heads comply with the YOLO's method. Two YOLO box detection branches connect to the downsampling network at the output of block ``res4" and ``res5", referred to Table \ref{tab-darknetlight}. Each branch has a private $3\times3$ convolution with 512 filters which useful for learning the spatial-resolution-specific feature at the particular feature map's size ($w \times w$, in Table \ref{tab-yolohead}). Then, their outputs connect to every ends which remaps tensor's depth to the number of anchors by pointwise convolutions for producing the final box predicted values. There are five box output ends, where fours are box coordinates ($t_x, t_y, t_w, t_h$) and one is for the objectness scores, detailed in Table \ref{tab-yolohead}. Three anchors are applied, where anchors' size are calculated on mean of samples of the dataset, and the rests are fixed at $\pm15\%$ aspect ratio distortions on the mean value.

According to Table \ref{tab-yolohead}, hyperbolic tangent function ($\mathrm{tanh}$) activates the offset values of anchors' centers. Sigmoid is applied for objectness scores. Anchors' dimension offsets are set to un-activated outputs.

\begin{table}[]
\centering
\caption{Structure of a YOLO detection branch with the three prior boxes or anchors, where $n$ represents the number of filters, and activation function is denoted by $f(x)$. $w$ and $d$ orderly represents input tensor's spatial size and its depth, which may come from ``res4" or ``res5".}
\label{tab-yolohead}
\resizebox{\linewidth}{!}{%
\begin{tabular}{@{}lccccc@{}}
\textbf{block} 		& ops & $n$ 	& \textbf{filter} 	& $f(x)$ & \textbf{output} 			\\ \midrule
\multicolumn{4}{l}{\makecell[l]{res4 ($w=14, d=384$),\\ res5 ($w=7, d=512$)}} 	&	& $w \times w\times d$      \\ \midrule
private  & Conv2d   & 512			& $3\times3$          & leaky           	& $w \times w\times512$      \\ \midrule
\multicolumn{6}{c}{Split}                                                                            \\ \midrule
$t_x$		& Conv2d       & 3          & $1\times1$             & tanh              & $w \times w \times 3$         \\
$t_y$		& Conv2d       & 3          & $1\times1$             & tanh              & $w \times w \times 3$         \\
$t_w$		& Conv2d       & 3          & $1\times1$             & -                   & $w \times w \times 3$         \\
$t_h$		& Conv2d       & 3          & $1\times1$             & -                   & $w \times w \times 3$         \\
obj		& Conv2d       & 3          & $1\times1$             & $\sigma$        & $w \times w \times 3$         \\ \bottomrule
\end{tabular}%
}
\end{table}

\subsubsection{Upsampling Path}
Upsampling path employs U-Net structure that low resoultion tensors are enlarged by transpose convolutions whose inputs are taken from same previous layers, shown in Fig \ref{fig1-modelA}. Following the U-Net convension, tensors in every spatial changes in backbone layers are gathered, by depth-concatenation operation, with one from previous transpose convolution for the next transpose convolution step. Activation functions for all layers of upsampling path are LeakyReLU as well as in the backbone network. Explaination in details is in Table \ref{tab-unetpath}.

\begin{table}[]
\centering
\caption{Configuration of U-Net-like Upsampling Path, where ops represent a transpose convolution path in Fig \ref{fig1-modelA}, and $n$ is the number of filters}
\label{tab-unetpath}
\begin{minipage}{\linewidth}
\begin{center}
\begin{tabular}{p{0.15\linewidth}p{0.15\linewidth}p{0.5\linewidth}}%>%{@{}lcr@{}}
ops 	& $n$ 	& \textbf{input}  					\\ \midrule
u1	&128	    & $7 \times 7 \times 512$              	        \\
u2	&128	    & $14 \times 14 \times (384+128)$    	     \\ 
u3	&64	    & $28 \times 28 \times (256+128)$    	       \\
u4	&64	    & $56 \times 56 \times (128+64)$      	        \\
u5	&32	    & $112 \times 112 \times (64+64)$    	        \\ \bottomrule
\end{tabular}%
\end{center}
\footnotetext{Note: All 2D transpose convolution are done with kernel's size $2\times2$ and stride 2, causing the enlarging factor of 2.}
\end{minipage}
\end{table}

\subsubsection{Pixel Classification Branch}
Three pixel prediction blocks (Pxl Header) are proposed, named ``Pxl Header1", ``Pxl Header2", and ``Pxl Header3" in Fig \ref{fig1-modelA}. Table \ref{tab-pxlheader} outlines the function layers on each Pxl Header, where all block has the same compoments. A tensor which size $224 \times 224 \times 64$, that comes from the last layer of the upsampling network, is simultaneously fed into every pixel prediction blocks. Table \ref{tab-pxlheader} presents the structure of Pxl Header. The results from every pixel prediction blocks are fed to the mapping function $\mathrm{g}(x)$ that produces tapping line pixels in a binary image.

\begin{table}[]
\centering
\caption{Components inside a pixel prediction block (Pxl Header), the executions of ops perform in the order}
\label{tab-pxlheader}
\begin{minipage}{\linewidth}
\begin{center}
\begin{tabular}{@{}clcc@{}}%{p{0.1\linewidth}p{0.2\linewidth}p{0.25\linewidth}p{0.25\linewidth}}
Order 	& ops 	& \textbf{input} $d$\footnotemark[1]  & \textbf{output} $d$\footnotemark[1]	\\ \midrule
1	& Pointwise Conv2d	& 64  & 1 \\
2	& Sigmoid Activation	& 1  & 1 \\
3	& Column-wise Softmax   & 1 & 1 \\ \bottomrule
\end{tabular}%
\end{center}
\footnotetext[1]{All tensor's spatial size equals to $224\times 224 \times d$}
\end{minipage}
\end{table}

Two-dimension convolution with filter size of $1\times1$ (also known as pointwise convolution) and filter's depth of 64 transforms the input tensor to be a size of $224\times 224 \times 1$.  Then Sigmoid activation modifies the useful values to be pass through. Pxl Header introduces the Column-wise Softmax layers. According to the tapping line definition that only one pixel in a column will contain a tapping line's pixel, applying a standard unit Softmax function on each image's column conforms such annotations. Sum-to-one property of Softmax adds a row-position dependency within an image's column, which means that it suppressed others while boosting one another. This process can be compared to the edge thinning algorithm of image-processing category, and also helps the model's training to be more likely converged faster. 

For inferencing a tapping line, function $\mathrm{g}(x)$ composed of three operations. Firstly, it finds the pixel location with the highest probability on each column (columnwise-argmax) of each input, then checks for the majorities (2 of 3) and binarizes the majority values on every pixels, and finally passes them to the pixel output node. 

In the network traning phase, pixel classification losses are computed on each outputs of pixel prediction blocks, which means function $\mathrm{g}(x)$ and beyonds are not contribute in the calculations. Learnable weights of pointwise convolution in a particular Pxl Header combine low-level and high-level features via any mapping function, which can be arbitary specified in network traning phase. Sigmoid function bounds the values in between 0 and 1, which controls the numerical stability on each neuron and enhances the training convergence.

\subsection{Training}
Dataset for this paper contains 1,425 tapping line images, only RGB1 version \cite{Wongtanawijit_2021} is used. Complying K-fold cross validation scheme with k=5, image data are divide into five folds. For each fold, there are 285 samples for evaluation and 1,140 samples for training.

\subsubsection{Dataset Priors and Statistics}
All RGB image are resized to network's input size, $224\times224\times3$ pixels with not preserving their original aspect ratios. An averaged size of annotated tapping line bounding boxes, with respected to the resized image, is 59.2 width and 70.6 height. Therefore, the values estimates for anchors' sizes are (59.2, 70.6), (50.2, 79.6), and (66.6, 60.0). These anchors will be constantly used across all the models.

\begin{figure}[h]%
\centering
\includegraphics[width=0.275\linewidth]{fig2.pdf}
\caption{Visualization of estimated anchors. All box's aspect ratios are in scale. Dashed line represents the averaged box}\label{fig2-anchors}
\end{figure}

Estimation of anchors' size also implies the unbalancing between tapping line's pixels and the background pixels. The annotation regulation of the tapping line (single pixel per column) gives the approximation ratio, by the anchor's width to the box's area : $59.2 / 224^2 \approx 0.0012$, which indicates highly imbalanced data between two classes.

\subsubsection{Branch Loss Function}
A benifit of one-stage network is that the calculation of losses happens at the output nodes. According to the model, output nodes are two YOLO heads, and three pixel prediction blocks (Pxl Headers). Thus, the total loss is the linear combination of them.

On both YOLO detection heads, MSE loss (L2-loss) is used for box's coordinates regression, and Dice-Coefficient scores the objectness predictions on the feature-map grids. For pixel classification loss, calculation on each pixel predicton block is handled by different loss functions. Dice-Coefficient, Binary Focal, and Tversky index with high $\beta$ are separately used on each Pxl Header, on account of highly imbalance between tapping line's pixels and background's pixels.

\subsubsection{Parameters Settings}
Training of the model on every folds shares same training parameters. Mini-batch gradient descent with momentum optimizer is used with mini-batch size of 4, and momentum coefficient constantly at 0.9. Training for a model processes in a range of 90 to 100 epochs, since the early-stopping may be applied in a particular model. Learning rates are scheduled by the WarmUp-Steady-Decay profile with a peak of 0.008, shown in Fig \ref{fig3-LR}. All the weights on the model are newly initialized. 

Unit-rescaling and z-score transform normalize the pixel values of an input image across all its depth dimension (color channel). Batch normalization is also appiled inside the layers that contain learnable weights for the training purpose.

\begin{figure}[h]%
\centering
\includegraphics[width=0.65\linewidth]{fig3.pdf}
\caption{WarmUp-Steady-Decay Learning Rate Profile}\label{fig3-LR}
\end{figure}

\subsubsection{Regularization}
To archieve learning regularization or to prevent the model from overfitting, a couple of tools, such as weight regularization, data augmentation, drop-out, are presented. Traditional L2-regularization is used on the weights of the downsampling network. Random rotations within $\pm20^\circ$ and the magnifications within $\pm20\%$ are applied on the input images for the augmentation purpose. Another tool is checking the box predictions of YOLO outputs. If the box predictions reach 0.8 IoU with the groundtruth, then the box's loss calculations in the paricular grid position of YOLO feature maps will be skipped.

\subsection{Evaluation Method}
The bounding box can justify the existence of a tapping line in the image, which is usually carried the box overlapping ratio, such as Jaccard Index (a.k.a IoU), Dice's coefficient, etc., between predictions and the annotated data. Box overlapping ratio does not reflect the similarity of the tapping line though \cite{Wongtanawijit_2021}. Pixel-level overlapping ratios or geometry distances function are required to measure the similarity or the error between two polygonal curves that encoded in image's pixels.

The bounding box overlapping ratios with 0.5 and 0.75 IoU levels determine average precisions of the detections. And, pixel-level Dice's coefficient (a.k.a F1-score) on the tapping line's pixels verifies the exact overlap between two polygonal's vertices. However, appling the overlapping measurement on the polygonal vertices in the pixel-level over penalizes the small mislocated pixels. As visualized in Fig \ref{fig4-polyline}, the predicted polyline has a small shift in distance to the reference line. This makes the Dice's index approaching to zero, whereas a 2-D geometric distance, Hausdorff distance, indicates strong similarity between them.

\begin{figure}[h]%
\centering
\includegraphics[width=0.5\linewidth]{fig4.pdf}
\caption{A drawing of two polylines that are locating in a discrete unit grid. Assuming that a intersection on the grid represents a pixel location in an image. A detected tapping line (dashed line) merely distances to the groundtruth (solid line) with 1 pixel is the exact overlapped.}\label{fig4-polyline}
\end{figure}

To clearify the calculation of both metrics, let any polygonal lines where thier vertices are on a discrete grid (positive integers, $\mathbb{I^+}$), which named the ``discrete curve". So, let $\textbf{A}$ and $\textbf{B}$ respectively represent the set of vertices of a predicted tapping line and its corresponding groundtruth. Dice's coefficient ($D$) is obtained in equation \ref{eq-dice}
\begin{equation}
D = \frac{2 \lvert \textbf{A} \cap \textbf{B} \rvert }{\lvert\textbf{A}\rvert+\lvert\textbf{B}\rvert}\label{eq-dice}
\end{equation}
Hausdorff distance ($\mathrm{d_H}$), measures for similarity between two curves, is defined by equation \ref{eq-hausdorff} \cite{Huttenlocher1993ComparingIU}, where $a \in \textbf{A}$ and $b \in \textbf{B}$. Function $\mathrm{d}(\textbf{A},\textbf{B})$ denotes for the directed Hausdorff distance (one-way) from curve \textbf{A} to curve \textbf{B}.
\begin{align}
\mathrm{d}(\textbf{A},\textbf{B}) &= \max_{a\in\textbf{A}} \min_{b\in\textbf{B}} \| a-b \|  \nonumber \\
\mathrm{d_H}(\textbf{A},\textbf{B}) &= \max (\mathrm{d}(\textbf{A},\textbf{B}), \mathrm{d}(\textbf{B},\textbf{A})) \label{eq-hausdorff}
\end{align}

Box annotation with a polyline inside enables the evaluations in many ways. This paper presents two modes of tapping line evaluations. The exact average calculates all evaluating metrics on testing data. Second, using the bounding boxes decides for the existance of a tapping line in the image, then calculates other metrics within the detected results. These different methods thoroughly describe the detection performances on each model, and this topic will be continued in Section \ref{sec-results}.

\section{Experiment Setup}
\subsection{Baseline Model and Variants}

\section{Results and Discussions}\label{sec-results}

\section{Conclusion}

Conclusions may be used to restate your hypothesis or research question, restate your major findings, explain the relevance and the added value of your work, highlight any limitations of your study, describe future directions for research and recommendations. 

In some disciplines use of Discussion or 'Conclusion' is interchangeable. It is not mandatory to use both. Please refer to Journal-level guidance for any specific requirements. 

\backmatter

\bmhead{Supplementary information}

If your article has accompanying supplementary file/s please state so here. 

Authors reporting data from electrophoretic gels and blots should supply the full unprocessed scans for key as part of their Supplementary information. This may be requested by the editorial team/s if it is missing.

Please refer to Journal-level guidance for any specific requirements.

\bmhead{Acknowledgments}

Acknowledgments are not compulsory. Where included they should be brief. Grant or contribution numbers may be acknowledged.

\section*{Declarations}

Some journals require declarations to be submitted in a standardised format. Please check the Instructions for Authors of the journal to which you are submitting to see if you need to complete this section. If yes, your manuscript must contain the following sections under the heading `Declarations':

\begin{itemize}
\item Funding
\item Conflict of interest/Competing interests (check journal-specific guidelines for which heading to use)
\item Ethics approval 
\item Consent to participate
\item Consent for publication
\item Availability of data and materials
\item Code availability 
\item Authors' contributions
\end{itemize}

\noindent
If any of the sections are not relevant to your manuscript, please include the heading and write `Not applicable' for that section. 

\bibliographystyle{bst/sn-basic}
\bibliography{sn-bibliography}

\end{document}
