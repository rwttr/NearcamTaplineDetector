%%\documentclass[referee,sn-basic]{sn-jnl}% referee option is meant for double line spacing
%%\documentclass[lineno,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style

\documentclass[default,pdflatex,iicol]{sn-jnl}% Default with double column layout

%%<additional latex packages if required can be included here>
\usepackage[numbers]{natbib}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{makecell}
\usepackage{placeins}
\usepackage{amssymb}

\jyear{2021}
\raggedbottom
%%\unnumbered% uncomment this for unnumbered level heads

\begin{document}

\title[Article Title]{Nearcam Tapline Detection 2}

\author[]{\fnm{Rattachai} \sur{Wongtanawijit}}\email{rattachai.gov@gmail.com}
\equalcont{These authors contributed equally to this work.}

\author*[*]{\fnm{Thanate} \sur{Khaorapapong}}\email{thanate.khao@gmail.com}
\equalcont{These authors contributed equally to this work.}

\affil*[1]{\orgdiv{Department of Computer Engineering, Faculty of Engineering}, \orgname{Prince of Songkla University}, \orgaddress{\street{P.O. Box 2 Kohong}, \city{HatYai, Songkhla}, \postcode{90112}, \country{Thailand}}}

\abstract{Current image-task-related convolutional neural network structures lean toward the directed acyclic graph that allows multiple output nodes. This enables a solution for the rubber tapping line detection that various desired output types, such as bounding boxes, pixel points or edges, are necessary. This paper demonstates that such deep networks with their outputs are bounding box and semantic pixel segmenation mask by adopting YOLO and U-Net layer structures. This paper proposes a column-wise argmax function with the technique of redundant mask outputs intended to enhance pixel classification accuracy. Experiments with the networks discover that some novel segmentation loss functions have different characters for tapping line prediction which were observed with geometric distance and F1-score. The redundant mask output can omits their weaknesses and yield higher detection accuracy compare to every single ones. Also, the column-wise argmax well suit to the tapping line detection in place of a thinning algorithm rather than the thresholding on the spatial output neurons.}

\keywords{\textit{hevea brasiliensis}, Semantic Edge, Rubber Tapping, Tapping Line, YOLO}

\maketitle

%% Section
\section{Introduction}\label{sec-introduction}

A self-instructed robot that has abilities to detect things and interact with, a sensory device with a detection algorithm is at least required. The detail of a vision system of a rubber tapping robot using a robotic arm, e.g. \cite{patent1, patent2, patent3, WANG2022103906, Zhou2021rubber}, which introduced in \cite{Wongtanawijit_2021}, adopted a near-range camera co-operating with specifically designed algorithm to perform tapping line detection. Continuing on the previous work, this paper presents another tapping line detection algoirthm in a whole different way. Given the definition of a tapping line in an image, is a piecewise polynomial where its vertices are at the pixels in the image grid \cite{Wongtanawijit_2021}. The detection is to precisely locate that pixels.

Motivations of this paper come from recent deep convolutional neural nets (CNN), such as object bounding-box detector, mask, edge-aware or semantic edge segmentation networks. But, none of them are ready-to-use solution for this problem. Original contributors of those works designed their networks to fit into more generic data than the tapping line images. This encourages this paper to explore other researches that answer how can those generic networks be adopted for some more specific datasets that might be useful for the tapping line detection.

Considering the essences of CNN-based object detectors architectures, many of them shared some common properties such as layer configurations. For example, the downstream convolution path shrinks down the input's spatial dimensions. In any arbitary layer orders, high-level detection which known as "classification" happens in low-resolution tensors, while other lower semantics such as edge detection are performed at higher tensor's resolution. This is because individual parts in CNN play the different roles. Some layers extract the useful information, while others shape that information to the desire outputs which specified by the loss functions. Notable differences are at the network's output or head. A box detector's output is the grid coordinates which represent the location and the dimension of the boxes, while the output of segmentation mask is the probability score at each pixels.

For the tapping line, this paper demonstates a network that composed of multiple different output types. Box outputs roughly determine where the tapping line is in a rectangular area. Pixel classification output answers where the polyline's vertices are wihtin the box. Network's structure are arranged in a directed acyclic computational graph in order to make the network trainable with gradient-based back-propagation optimizer with the end-to-end training scheme. A pixel classification head obtains the data from the downsampling-upsampling path likewise in U-Net \cite{unet}, V-Net \cite{vnet} or other pixel-level segmentation networks. Co-operating with the proposed column-wise argmax function, the networks can directly produce tapping lines without any post-processings. The network calculates the boxes' coordinates at two prediction heads with the earlier YOLO \cite{redmonyolov2} 's regression form.

Training to detect tapping lines, a set of novel loss for pixel classfication on very unbalanced data, Dice-Coefficient, Binary Focal, and Tversky indices are inspected for loss-value convergence and the prediction accuracy. The overall loss for the network is the combination of YOLO's box regression and pixel classification losses. Assessing the network's performance, a set of evaluation metrics are applied to the predictions. The Intersection-Over-Union (IoU) ratios of the box clearify the existence of a tapping line in an image. F1-score and distance errors describe the closeness of the predicted pixels to the actual tapping line inside the box.

Detection results appear to be diverse when the network are trained with different pixel classification loss. There are some trade-off between F1-score and Hausdorff distance, but all is on par with each others. Every models provide above 92.0 percents in average precisions with bounding box evaluation. The results also point that by the appending of column-wise Softmax to the pixel prediction head helps the networks to gain more accuracy than the widely used pixel-wise activated neurons. 

In order to archieve the higher detection accuracy, this paper also presents a modification in top layers of pixel classification head for separately predict multiple loss or deep supervising, then gather all of them by voting for the pixel majorities with the use of column-wise Softmax function. The proposed method can generate the results in higher F1-score with also diminishes the distance between two polylines.

The rests of this paper include: the review of literatures in Section \ref{sec-review} \textemdash e.g. the previous work of the tapping line detection, relevant applications of muliple output node CNN detectors, etc. The design of tapping line detection network in Section 3. The experiment setup and results in Section 4 and Section 5. Summary with the disscussions are in Section 6.

\section{Background and Motivation}\label{sec-background}
The purpose of the near-range vision camera is for precisely locate the tapping position for tapping manipulation of the robot arm. Our previous work \cite{Wongtanawijit_2021} demonstrated the detection algorithm in a form of an image processing algorithm. The method produces accurate results but it was specially designed and required two input images accommodating by artificial lighting patterns for the nighttime farming. Apart from this paper that, the CNN-based detectors can learn what and where is the tapping line under any lighting circumstance depends on the training dataset. Moreover, tappling line detection by modern computer vision algorithms is not yet explored at this time. Hence, it is a interesting topic to study the capable of CNN-based detectors with the tappline line image dataset, and might be useful as an alternative algorithm for the robot.

%% Section
\section{Review of Literatures}\label{sec-review}
\subsection{Tapping Line Detection}
Rubber tapping is a process of farming the natural rubber latex from a ParÃ¡ rubber tree (\textit{Hevea brasiliensis}). Tapping is making a cut on the rubber tree bark or the tapping panel letting the latex to flow along. This cut is leaving a gutter that called the tapping path. Refer to the rubber tapping document \cite{abraham1992tapping}, the cut is recommended to be 30 to 45 degrees angle to the ground axis. Likewise, small angle tolerances are acceptable. A robot with a robot arm integrated, which able to do tapping automatically, probably have been installed a close-up camera for fine detail acquisition as in many visual-servoing systems (patents \cite{patent1, patent2, patent3}). Consequently, the appearance of a tapping path is a certain line in the image which named the "tapping line".

The image data with labels are necessary for developing a machine learning algorithm by the supervised learning. The previous work \cite{Wongtanawijit_2021} labels a tapping line by two annotations. As shown in Fig \ref{fig-sampleannotation}, a bounding box encodes the tapping endpoints that to be at the top-left and the bottom-right corner of the box. Then, a polyline or a piecewise polynomial connect to its corners as the tapping line endpoints. As a results, the polyline's vertices can be defined on every columns inside the box for the annotation convenience and satisfying the spiral downward tapping system.

\begin{figure}[h]%
\centering
\includegraphics[width=\linewidth]{sample_annotation.pdf}
\caption{A near-range tapping panel image (left) and the magnified cropped (right). The tapping line is annotated by a bounding box (dotted green rectangle) and a polyline ( multiple segments in red along the box's diagonal).}\label{fig-sampleannotation}
\end{figure}

\subsection{Overview of Deep Convolutional Neural Networks}
The development of new substantial ideas on deep CNN-based detectors quite becomes settled in recent years. Back then when CNN-based box detectors, Faster-RCNN \cite{fasterrcnn} and YOLO \cite{redmonyolov1}, were established in 2015. Much more recent detectors such as YOLOv3 \cite{redmonyolov3} (2018), YOLOv4 \cite{alexyyolov4} (2020) able to detect any objects in the images with such very high rates than their predecessors. Later researches put lots of improvements on the modern networks. For some details, YOLOv3 and v4 have multiple prediction head that works on different spatial resolution so that the detector better locates small objects in previous YOLO and even better than the Faster-RCNN. In the later version, YOLOv4, exploits the path skipping in various stages with dense and sparse prediction layers. This make YOLOv4 push the network produce the better results. But, the mechanism of bounding box's coordinate prediction in YOLOv3 and YOLOv4 still the same. Nonetheless, the aim of the improvements is to make the network to learn or to discover the features of the data as much as possible. Since the datasets that used in those detector development can be seen as a generic dataset \textemdash a dataset that an image may contain various numbers of object instances and object classes. 

Apart from boxes, there are other networks that can predict examining the image in pixel level as well. Some novel networks such as U-Net \cite{unet}, V-Net \cite{vnet}, SegNet \cite{segnet}, etc., produce a mask from the image as the segmentation task in image processing category. These models separate the areas of the objects (or other interests) from the background. These models employ a chain of upsampling or transposed convolution layers to enlarge the spatial size of the data back to input's size, which are suitable for fine pixel detection application.

Another example is the semantic-aware prediction. The networks not only mark the object into the pixel area but also recognize which classes of each pixels in the mask are associate with. Remarkable examples are CASENet \cite{casenet}, Mask-RCNN \cite{maskrcnn}, etc. However, these networks are build on the purpose of the generic datasets as mentioned earlier. Lots of network architecture are proposed and indicated the powerful of convolutional networks on the computer vision tasks. It remains a challenge to adopt those networks addressing some applications in specific domains.

\subsection{Network Loss Function}\label{review_loss}
Mapping of values between responses (values returned from a network) with expectations (target values) is the core principle of any neural network. The difference  between targets and responses, which calculated by a function, is a loss, and the such function is named ``loss function". Many researches on CNN-based detectors present loss functions that refined for their particular problems. For instances, Dice-Coefficient index \cite{vnet, gendice}, well robustly handle a segmentation task in the medical imaging. Focal loss \cite{focalloss1}, an extension of cross-entropy function, is appropriate for a class-imbalanced data. Tversky index \cite{tverskyloss} also reduces the effects of unbalanced data to be learned by the segmentation network.

By understanding the nature of the data that they are working on especially in the segmentation problems, many researches did some mathematical adjustments on the exist loss functions, letting their networks produce preferable results. Focal loss \cite{focalloss1} reformulates binary classic binary cross-entropy function by add the extra weight on $\gamma$ term to address the ``hard example" not only foreground-background imbalance, since they foreseen their dataset have included some problematic samples. As well as the increment of $\beta$ term of Tversky index \cite{tverskyloss} preventing the network to over-learn by the false-positive samples (improve recall), because their interested points are very sparse in amount and space compared to the background. So, selecting a loss function for a network in a particular problem is crucial for network's performance not only the layer configuration of the network.

Meanwhile, the losses of bounding box's coordinates regression in all of the YOLOs and R-CNN variants, stay on L-norm functions. Discussion in the performance of L2-norm and L1-norm indicates that the regression with L1 is more robust than L2 in loss dynamics. And L2 tends to cause more numerical instability issue than L1 does, so that it should used with a small learning rate \cite{fastrcnn}.

\subsection{Deep Learning Computer Vision in Agricultural Robotics}
Conducting the powerful deep CNN-based computer vision to agricultural robotics becomes popular in following years after mentioned algorithms are established. Lots of them focused on ripe fruit detections in robotized harvesting by adapting detection algorithms to facilitate their objectives. For instance in tomato detections with YOLO, Guoxu Liu et al. \cite{s20072145} changed the bounding box output of YOLOv3 to the ``circular bounding box" with a radius regression equation which agrees with a circular shape of tomatoes, while \cite{Lawal_2021, chentomato} stay on rectangular bounding boxes and add new layer configurations to achieve higher detection accuracy than the standard YOLO Darknet-53. There are also a lot of examples for this category to be reviewed later in the following section such as, apple detection and segmentation with U-Net \cite{Li2021apple}, Mask-RCNN \cite{Chu2021apple, Jia2020apple}, YOLOv3 \cite{Kuznetsova2020apple}, Mask-RCNN on strawberry \cite{Yu2019strawberry}, etc.

Inspecting for details on the mentioned works that how they did the modifications on standard detector networks, a popular mod is to add multi-scale predictions to the network (a.k.a feature-pyramid \cite{Tong2020}) in order to enhance the recognition of object in various sizes, especially for discover small objects. This technique add dedicate paths in the network graph, which are found in such as \cite{chentomato, Lawal_2021, Yu2019strawberry, Li2021apple}. Researches of such as \cite{Jia2020apple, Chu2021apple, Yu2019strawberry, Lawal2021tomato} employ the adventages of feature re-use that presented in densely-convolution backbone \cite{densenet} or the residual-block of ResNet \cite{resnet}, which efficiently increase the network learning capability. Mish activation function is more prefered over ReLU family in order to improve network training dynamics and final accuracy, but this might comes with more computational expense \cite{misra2019mish}.

In segmentation applications, downsampling upsampling path of U-Net already included the feature-pyramid structure. \cite{Li2021apple} tackles bluring problem of input images by add gated-convolution between each spatial resolution changes in U-Net. Suppression Mask-RCNN \cite{Chu2021apple} appends another convolutional network with an additional window sliding operation to the end of Mask-RCNN for filtering unwanted features which causes the overall network becomes a longer chain and requires a separate training scheme.

All of those mentioned works decided to stick with standard loss functions from original papers, multi-class or binary cross entropy losses for classification, L1-smooth for RCNN's box, and averge of pixelwise cross-entropy for segmentation loss. They rarely show the use of other losses, e.g. loss that designed for imbalanced data, as mentioned in section \ref{review_loss}. This is because labels of their datasets rather be in the balanced shape so they can ignore this regard.

\subsection{Computer Vision for Rubber Latex Harvesting}
Maneuvering a robot approaches a rubber tree in a near-range, likewise in \cite{zhang2019rubber, WANG2022103906}, with aiming a onboarded camera to recoginze the position of a tapping path in the tapping panel. The tapping line's appearance in the image can be considering as an edge or a curve line that trace along the trunk curvature. Thus, detection of a tapping line falls into a category of semantic edge detection \textemdash detect edges that be a part of an object. This unlikes from detecting a solid object or a closed-contour shape in the fruit picking sences. For example, training the model for tapping line detection might be facing more data imbalanced issues because edges' pixels are much less than the backgound. A bounding box that placed onto a tapping line encodes tapping endpoint information and a tapping line edge is a polygonal line, as decribed in \cite{Wongtanawijit_2021}. Therefore, a tapping line detector might include a box detector with a segmentation branch. The implementations have high tendency to use YOLO over R-CNN variants due to efficiency, low computational cost which is feasible for a mobile platform.


%% Section
\section{Methodology}\label{sec-methodology}


\subsection{Model}\label{subsec-model}
Fig \ref{fig1-modelA} illustrates the overview of the tapping line detection network in a block diagram. The computational graph consists of a downsampling backbone network for extracting features, two box detection heads at different spatial resolutions for object size coverages, a upsampling path that enlarges tensors' size back to the input's size, and a pixel classification head for the detection of tapping line's pixels. This one-staged design facilitates the end-to-end training. Since, tapping line detection is a one-class object detection problem and likely to be deployed on a mobile robot. Hence, a small or compact network, in term of numbers of weights and data's sizes, is favourable.

\begin{figure*}[h]%
\centering
\includegraphics[width=\textwidth]{fig1.pdf}
\caption{The proposed tapping line detection network, visualized in a directed grpah with blocks. Network composes of four parts, from left-to-right are Downsampling network, two YOLO detection branches, Upsampling network, and Pixel classification branch.}\label{fig1-modelA}
\end{figure*}

\subsubsection{Downsampling Network}
The downsampling network borrows residual block structures of Darknet-53. The modifications are trimmed out the numbers of filters, shrunk the overall network depth and reduced the input size to $224 \times 224 \times 3$\footnote[1]{tensor's size naming convension given by width $\times$ height $\times$ depth, and also applied across this paper}, while preserving filter's sizes and LeakyReLU activation function, as shown in Table \ref{tab-darknetlight}. This network path is shared as a feature extractor backbone among two YOLO detection branchs, the upsampling path, and the pixel classification branch.

\begin{table}[]
\centering
\caption{Downsampling backbone network explained in block structures with given names (\textbf{block}). The denoted labels are layer operation (ops), a 2-dimension convolution (Conv2d), a shortcut connection with addition (Residual), $s$: convolution stride, $n$: numbers of filters, filter kernel's size (\textbf{filter}), and $c$ is the numbers of block replication.}
\label{tab-darknetlight}
\resizebox{\linewidth}{!}{%
\begin{tabular}{lcccccc}
\textbf{block}      & $c$ 			  & ops 		& $n$   		& \textbf{filter} 	& $s$ 	& \textbf{output} 	\\ \midrule
Input                 &                    & -                       			&    		&       				&  	& $224\times224\times3$   \\ \midrule
                      	&                    & Conv2d                  		& 32 		& $3\times3$   	& 1 	& $224\times224\times32$  \\
down1               	&                    & Conv2d                  		& 64  	& $3\times3$    	& 2 	& $112\times112\times64$  \\ \midrule
\multirow{3}{*}{res1} & \multirow{3}{*}{1} & Conv2d            & 32  	& $1\times1$    	& 1 	&             	\\
                      	&                    & Conv2d                  		& 64  	& $3\times3$    	& 1 	&   \\
                      	&                    & Residual             			&     		&        			  	&   	& $112\times112\times64$  \\ \midrule
down2             	&                    & Conv2d                  		& 128 	& $3\times3$    	& 2 	& $56\times56\times128$   \\ \midrule
\multirow{3}{*}{res2} & \multirow{3}{*}{2} & Conv2d            & 64  	& $1\times1$    	& 1 	&      \\
                      	&                    & Conv2d                  		& 128 	& $3\times3$    	& 1 	&   	\\
                      	&                    & Residual             			&     		&        			  	&   	&	$56\times56\times128$   \\ \midrule
down3               	&                    & Conv2d                  		& 256 	& $3\times3$    	& 2 	&	$28\times28\times256$   \\ \midrule
\multirow{3}{*}{res3} & \multirow{3}{*}{4} & Conv2d            & 128 	& $1\times1$    	& 1 	&      \\
                      	&                    & Conv2d                  		& 256 	& $3\times3$    	& 1 	&    	\\
                      	&                    & Residual             			&     		&        				&   	& $28\times28\times256$   \\ \midrule
down4                &                    & Conv2d                  		& 384 	& $3\times3$    	& 2 	& $14\times14\times384$   \\ \midrule
\multirow{3}{*}{res4} & \multirow{3}{*}{4} & Conv2d            & 192 	& $1\times1$    	& 1 	&      \\
                      	&                    & Conv2d                  		& 384 	& $3\times3$    	& 1 	&    	\\
                      	&                    & Residual             			&     		&        				&   	& $14\times14\times384$   \\ \midrule
down5               	&                    & Conv2d                  		& 512 	& $3\times3$    	& 2 	& $7\times7\times512$     \\ \midrule
\multirow{3}{*}{res5} & \multirow{3}{*}{1} & Conv2d            & 256 	& $1\times1$    	& 1 	&      \\
                      	&                    & Conv2d                  		& 512 	& $3\times3$    	& 1 	&      \\
                      	&                    & Residual             			&     		&        				&   	& $7\times7\times512$     \\ \bottomrule
\end{tabular}%
}
\end{table}

\subsubsection{Box Detection Branches}
Box detection heads comply with the YOLO's method. Two YOLO box detection branches connect to the downsampling network at the output of block ``res4" and ``res5", referred to Table \ref{tab-darknetlight}. Each branch has a private $3\times3$ convolution with 512 filters which useful for learning the spatial-resolution-specific feature at the particular feature map's size ($w \times w$, in Table \ref{tab-yolohead}). Then, their outputs connect to every ends which remaps tensor's depth to the number of anchors by pointwise convolutions for producing the final box predicted values. There are five box output ends, where fours are box coordinates ($t_x, t_y, t_w, t_h$) and one is for the objectness scores, detailed in Table \ref{tab-yolohead}. Three anchors are applied, where anchors' size are calculated on mean of samples of the dataset, and the rests are fixed at $\pm15\%$ aspect ratio distortions on the mean value.

According to Table \ref{tab-yolohead}, hyperbolic tangent function ($\mathrm{tanh}$) activates the offset values of anchors' centers. Sigmoid is applied for objectness scores. Anchors' dimension offsets are set to un-activated outputs.

\begin{table}[]
\centering
\caption{Structure of a YOLO detection branch with the three prior boxes or anchors, where $n$ represents the number of filters, and activation function is denoted by $f(x)$. $w$ and $d$ orderly represents input tensor's spatial size and its depth, which may come from ``res4" or ``res5".}
\label{tab-yolohead}
\begin{minipage}{\linewidth}
\begin{tabular}{lccccc}
\textbf{block} 		& ops & $n$ 	& \textbf{filter} 	& $f(x)$ & \textbf{output} 			\\ \midrule
\multicolumn{4}{l}{\makecell[l]{res4 ($w=14, d=384$),\\ res5 ($w=7, d=512$)}} 	&	& $w \times w\times d$      \\ \midrule
private  & Conv2d   & 512			& $3\times3$          & leaky           	& $w \times w\times512$      \\ \midrule
\multicolumn{6}{c}{Split}                                                                            \\ \midrule
$t_x$		& Conv2d       & 3          & $1\times1$             & tanh              & $w \times w \times 3$         \\
$t_y$		& Conv2d       & 3          & $1\times1$             & tanh              & $w \times w \times 3$         \\
$t_w$		& Conv2d       & 3          & $1\times1$             & -                   & $w \times w \times 3$         \\
$t_h$		& Conv2d       & 3          & $1\times1$             & -                   & $w \times w \times 3$         \\
obj		& Conv2d       & 3          & $1\times1$             & $\sigma$        & $w \times w \times 3$         \\ \bottomrule
\end{tabular}
\end{minipage}
\end{table}

\subsubsection{Upsampling Path}
Upsampling path employs U-Net structure that low resoultion tensors are enlarged by transpose convolutions whose inputs are taken from same previous layers, shown in Fig \ref{fig1-modelA}. Following the U-Net convension, tensors in every spatial changes in backbone layers are gathered, by depth-concatenation operation, with one from previous transpose convolution for the next transpose convolution step. Activation functions for all layers of upsampling path are LeakyReLU as well as in the backbone network. Explaination in details is in Table \ref{tab-unetpath}.

\begin{table}[]
\centering
\caption{Configuration of U-Net-like Upsampling Path, where ops represent a transpose convolution path in Fig \ref{fig1-modelA}, and $n$ is the number of filters}
\label{tab-unetpath}
\begin{minipage}{\linewidth}
\begin{center}
\begin{tabular}{p{0.15\linewidth}p{0.15\linewidth}p{0.5\linewidth}}%>%{@{}lcr@{}}
ops 	& $n$ 	& \textbf{input}  					\\ \midrule
u1	&128	    & $7 \times 7 \times 512$              	        \\
u2	&128	    & $14 \times 14 \times (384+128)$    	     \\ 
u3	&64	    & $28 \times 28 \times (256+128)$    	       \\
u4	&64	    & $56 \times 56 \times (128+64)$      	        \\
u5	&32	    & $112 \times 112 \times (64+64)$    	        \\ \bottomrule
\end{tabular}%
\end{center}
\footnotetext{Note: All 2D transpose convolution are done with kernel's size $2\times2$ and stride 2, causing the enlarging factor of 2.}
\end{minipage}
\end{table}

\subsubsection{Pixel Classification Branch}
Three pixel prediction blocks (Pxl Header) are proposed, named ``Pxl Header1", ``Pxl Header2", and ``Pxl Header3" in Fig \ref{fig1-modelA}. Table \ref{tab-pxlheader} outlines the function layers on each Pxl Header, where all block has the same compoments. A tensor which size $224 \times 224 \times 64$, that comes from the last layer of the upsampling network, is simultaneously fed into every pixel prediction blocks. Table \ref{tab-pxlheader} presents the structure of Pxl Header. The results from every pixel prediction blocks are fed to the mapping function $\mathrm{g}(x)$ that produces tapping line pixels in a binary image.

\begin{table}[]
\centering
\caption{Components inside a pixel prediction block (Pxl Header), the executions of ops perform in the order}
\label{tab-pxlheader}
\begin{minipage}{\linewidth}
\begin{center}
\begin{tabular}{@{}clcc@{}}%{p{0.1\linewidth}p{0.2\linewidth}p{0.25\linewidth}p{0.25\linewidth}}
Order 	& ops 	& \textbf{input} $d$\footnotemark[1]  & \textbf{output} $d$\footnotemark[1]	\\ \midrule
1	& Pointwise Conv2d	& 64  & 1 \\
2	& Tanh Activation	& 1  & 1 \\
3	& Column-wise Softmax   & 1 & 1 \\ \bottomrule
\end{tabular}%
\end{center}
\footnotetext[1]{All tensor's spatial size equals to $224\times 224 \times d$}
\end{minipage}
\end{table}

Two-dimension convolution with filter size of $1\times1$ (also known as pointwise convolution) with filter's depth of 64 transforms the size of input tensor to $224\times 224\times 1$. Then the hyperbolic tangent (tanh) activation modifies the values. Pxl Header introduces the Column-wise Softmax layers. According to the tapping line definition that only one pixel in a column contains a tapping line's pixel, so applying a standard unit Softmax function on each image's column conforms such annotations. Sum-to-one property of Softmax adds the row-position dependency within the image's columns, which means it suppressed others while boosting one another. This process can be compared to an edge-thinning algorithm of image-processing, and also helps the model's training to be more likely converged faster. 

For inferencing a tapping line, function $\mathrm{g}(x)$ composed of three operations, that run in the order by,
\begin{enumerate}
\item finds the pixel locations which have the highest probability on each column (columnwise-argmax) of each input.
\item checks for the majorities, 2 of 3, on pixel location from step 1.'s results. (Voting Operation)
\item binarizes the majority locations (set one for majority pixels, otherwise set to zeros)
\end{enumerate}

In the network traning phase, pixel classification losses are computed on each outputs of pixel prediction blocks, which means function $\mathrm{g}(x)$ and beyonds are not contribute in the calculations. Learnable weights of pointwise convolution in a particular Pxl Header combine low-level and high-level features via any mapping function, which can be arbitary specified in network traning phase. Sigmoid function bounds the values in between 0 and 1, which controls the numerical stability on the neurons and enhances the training convergence.

\subsection{Model Training}
Dataset for this paper contains 1,425 tapping line images, only RGB1 version \cite{Wongtanawijit_2021} is used. Complying K-fold cross validation scheme with k=5, image data are divided into five clusters (folds). In each fold, there are 285 samples for evaluation and 1,140 samples for training the model.

\subsubsection{Dataset Priors and Statistics}
All images in the dataset is collected within the nighttimes using artificial light souces. The camera produces tapping panel images with the dark background. And, an tapping panel image contains only one tapping line.

All RGB image are resized to network's input size, $224\times224\times3$ pixels with not preserving their original aspect ratios. An averaged size of annotated tapping line bounding boxes, with respected to the resized image, is 59.2 width and 70.6 height. Therefore, the values estimates for anchors' sizes are (59.2, 70.6), (50.2, 79.6), and (66.6, 60.0). These anchors will be constantly used across all the models.

\begin{figure}[h]%
\centering
\includegraphics[width=0.25\linewidth]{fig2.pdf}
\caption{Visualization of the estimated anchors. All box's aspect ratios are in scale. Dashed line represents the averaged box}\label{fig2-anchors}
\end{figure}

Estimation of anchors' size also implies the unbalancing between tapping line's pixels and the background pixels. The annotation regulation of the tapping line (single pixel per column) gives the approximation ratio, by the anchor's width to the box's area : $59.2 / 224^2 \approx 0.0012$, which indicates highly imbalanced data between two classes.

\subsubsection{Branch Loss Function}\label{subsubsec-pxlbranch}
A benifit of one-stage network is that the calculation of losses happens at the output nodes. According to the model, output nodes are two YOLO heads, and three pixel prediction blocks (Pxl Headers). Thus, the total loss is the linear combination of them.

MSE loss (L2-loss) is used for box's coordinates regression on both YOLO detection heads, and Dice-Coefficient scores the objectness predictions on the feature-map grids. For the pixel classification loss, calculation on each pixel predicton block is handled by different loss functions. Dice-Coefficient, Binary Focal, and Tversky index with high $\beta$  are separately used on each Pxl Header, on account of highly imbalance between tapping line's pixels and background's pixels. Since, adding more weight on $\beta$ in Tversky's formular \cite{tverskyloss} will increase the loss when tapping line pixels are predicted as background, which are the false-negative case.

\subsubsection{Parameters Settings}\label{subsubsec-trainingparams}
Training of the model on every folds shares same training parameters. Mini-batch gradient descent with momentum optimizer is used with mini-batch size of 4, and momentum coefficient constantly at 0.9. Training for a model processes in a range of 90 to 100 epochs, since the early-stopping may be applied in a particular model. Learning rates are scheduled by the WarmUp-Steady-Decay profile with a peak of 0.008, shown in Fig \ref{fig3-LR}. All the weights on the model are newly initialized. 

Unit-rescaling and z-score transform normalize the pixel values of an input image across all its depth dimension (color channel). Batch normalization is also appiled inside the layers that contain learnable weights for the training purpose.

\begin{figure}[h]%
\centering
\includegraphics[width=0.66\linewidth]{fig3.pdf}
\caption{WarmUp-Steady-Decay Learning Rate Profile. Learning rate grows and drops in exponential rates in the warm-up and the decay periods.}\label{fig3-LR}
\end{figure}

\subsubsection{Generalization}
Training the model adopts a couple of tools, such as weight regularization, image augmentation, weight drop-out, etc., to prevent the model from overfitting. L2 regularization\footnote[2]{Concerning the stabillity of training dynamics with single precision unit (fp32), the regularization factor is set to $2/m$ where $m$ is the number of learnable weights} is used on the learnable weights in all network's layers. Random rotations within $\pm20^\circ$ and the magnifications within $\pm20\%$ are applied on the input images for the augmentation purpose. Another tool is checking the box predictions of YOLO outputs. If the box predictions reach 0.75 IoU or highers with the groundtruth, then the box's loss calculations in that paricular spatial position on YOLO feature maps will be skipped.

\subsection{Model Inferencing}
Box detection branches and the pixel classification branch separately produce the outputs. Pixel classification end predicts a binary image that the valid pixels (1's) may be outside the predicted box from YOLO branch. To connect them together with agrees to the dataset that an image has only one tapping line, the network is going to produce one final bounding box as well.

For producing bounding box prediction, the network selects only one strongest prediction for an anchor size per YOLO branches, which means there are 3 predictions at a YOLO branch. Six predictions from 2 YOLO branches are therefore reduced to one final result by Non-maximal suppression (NMS).

At the pixel classication branch, function $\mathrm{g}(x)$ creates a binary image that contains one valid pixel (1's) per column. Some valid pixels may aligned with the tapping line while some are not. So, the prediction is to crop the binary image by the mentioned bounding box from NMS to filter out the unwanted pixels (boundary condition dominated by the box, valid pixels on the box's boderline are removed).

Accomplishing the meaningful result from both annotations, the final polyline will have the first vertex at box's top-left corner as a tapping line starting point, and the terminate vertex at box's bottom-right corner. Fig \ref{sampleagreement} illustrates an example of the process. This agreement is for inferencing a tapping line from the model.
% sample_tapline_box_agreement
\begin{figure}[h]%
\centering
\includegraphics[width=0.52\linewidth]{sample_tapline_box_agreement.pdf}
\caption{A drawing shows a polyline and a box that are locating in a discrete unit grid. Assuming that a intersection on the grid represents a pixel location in an image. The red box represents the NMS-processed box with a plolyline (top). The result that the polyline are cropped and re-locate vectices to the box's corners is in bottom.
}\label{fig4-sampleagreement}
\end{figure}

\subsection{Evaluation Method}
The bounding box can justify the existence of a tapping line in an image, using the boxes overlapping ratios between predictions and priors such as Jaccard Index (a.k.a IoU), Dice's coefficient, etc. Box overlapping ratios nonetheless do not reflect the similarity of the tapping line \cite{Wongtanawijit_2021}. Pixel-level overlapping ratios or geometry distances can measure for similarities or distance errors between two polygonal curves that encoded in image's pixels.

The bounding box overlapping ratios with 0.5 and 0.75 IoU levels determine average precisions of the detections. And, pixel-level Dice's coefficient (a.k.a F1-score) on the tapping line's pixels verifies the exact overlap between two polygonal's vertices. However, appling the overlapping measurement on the polygonal vertices in the pixel-level over penalizes the small mislocated pixels. As visualized in Fig \ref{fig4-polyline}, the predicted polyline has a small shift in distance to the reference line. This makes the Dice's index approaching to zero, whereas a 2-D geometric distance, Hausdorff distance, indicates strong similarity between them.

\begin{figure}[h]%
\centering
\includegraphics[width=0.4\linewidth]{fig4.pdf}
\caption{A drawing of two polylines in a discrete unit grid. Assuming that a intersection on the grid represents a pixel location in an image. A detected tapping line (dashed line) merely distances to the groundtruth (solid line) with 1 pixel is the exact overlapped.}\label{fig4-polyline}
\end{figure}

To clearify the calculation of both metrics, let any polygonal lines where thier vertices are on a discrete grid (positive integers grid, $\mathbb{I^+}\times \mathbb{I^+}$), which also known as the ``discrete curve". So, let $\textbf{A}$ and $\textbf{B}$ respectively represent the set of vertices of a predicted tapping line and its corresponding groundtruth. Dice's coefficient ($D$) is obtained in equation \ref{eq-dice}
\begin{equation}
D = \frac{2 \lvert \textbf{A} \cap \textbf{B} \rvert }{\lvert\textbf{A}\rvert+\lvert\textbf{B}\rvert}\label{eq-dice}
\end{equation}
Hausdorff distance ($\mathrm{d_H}$), measures for similarity between two curves, is defined by equation \ref{eq-hausdorff} \cite{Huttenlocher1993ComparingIU}, where $a \in \textbf{A}$ and $b \in \textbf{B}$. Function $\mathrm{d}(\textbf{A},\textbf{B})$ denotes for the directed Hausdorff distance (one-way) from curve \textbf{A} to curve \textbf{B}.
\begin{align}
\mathrm{d}(\textbf{A},\textbf{B}) &= \max_{a\in\textbf{A}} \min_{b\in\textbf{B}} \| a-b \|  \nonumber \\
\mathrm{d_H}(\textbf{A},\textbf{B}) &= \max (\mathrm{d}(\textbf{A},\textbf{B}), \mathrm{d}(\textbf{B},\textbf{A})) \label{eq-hausdorff}
\end{align}

In addition to the mentioned box and polyline evaluation metrics, this paper also includes the L2-distance ($\mathrm{d_{ep}}$) of tapping endpoints where two the tapping endpoints locate at the top-left and bottom-right corners of the box. This measurement adds more details on the box's location over IoU ratios. Let a predicted box is in a form of $[x_p, y_p, w_p, h_p]$ and the groundtruth box is $[x_g, y_g, w_g, h_g]$. Then, the predicted tapping endpoints are $ep_{p1} = (x_p, y_p)$ and $ep_{p2} = (x_p+w_p, y_p+h_p)$. Likewise, the groundtruth tapping endpoints are $ep_{g1} = (x_g, y_g)$ and $ep_{g2} = (x_g+w_g, y_g+h_g)$. The average endpoint distance is defined by equation \ref{eq-endpoint}.
\begin{equation}
\mathrm{d_{ep}} = \frac{1}{2} (\| ep_{p1}- ep_{g1} \| + \| ep_{p2}  - ep_{g2} \|) \label{eq-endpoint}
\end{equation}

Annotation with a box and a polyline enables the evaluations in many combinations. This paper presents a method by using the bounding boxes to decide for the existance of a tapping line in the image, then calculates other metrics within the detected results.

\section{Experiment Setup}\label{sec-exp}
The design of the model in section \ref{subsec-model} follows YOLOv3 and U-Net architectures, whereas the proposed modifications are made at output branches on the assumption that the modfications contribute to the model's prediction accuracy. Briefly, the proposed pixel classification branch, Fig \ref{fig5-proposedPxlHeader}, employs three parallel blocks (Pxl Header) for mapping between prediction and data by three novel segmentation loss functions for highly imbalanced data, Dice's, Focal and Tversky's index, with the auxiliary column-wise Softmax. Lastly, function $\mathrm{g}(x)$ attains incoming data from those blocks, producing the final binary mask. These components are critical for producing tapping line's pixels.

\begin{figure}[h]%
\centering
\includegraphics[width=0.55\linewidth]{fig5.pdf}
\caption{Pixel classification branch of the proposed model}\label{fig5-proposedPxlHeader}
\end{figure}

The proposed components can be summarized as.
\begin{itemize}
  \item Column-wise Softmax in Pxl Header.
  \item Parallel Pxl Headers with function $\mathrm{g}(x)$.
  \item Multiple segmentation loss for training the model.
\end{itemize}

Those assumptions require reference models or the baseline models in order to observe the components' contributions. The baseline model construction is piecewise shuffling those components in the model chain, such as reducing the amount of Pxl Header, bypassing the column-wise Softmax, etc. As a result, the experiment gives the name of the models by,
\begin{itemize} 
\item[] \emph{\textbf{Model A}}: The proposed model, as presented in Section \ref{subsec-model}. \\
\item[] \emph{\textbf{Model B}}: Similar to Model A but column-wise softmax functions are removed from all Pxl Headers. \\
\item[] \emph{\textbf{Model C}}: Made changes on Model A by,
	\begin{itemize}
  	\item Remove the parallel structure of Pxl Header, only a single block is used as shown in Fig \ref{fig6-baselinePxlHeader}. Pxl Header's structure is similar to one of the proposed model. 
  	\item Only one segmentation loss is used at the pixel classification output for training the model at a time.
  	\item Re-define $\mathrm{g}(x)$ by removing the voting operation from the original (in Section \ref{subsubsec-pxlbranch}), because there is only one incoming signal from Pxl Header.
	\end{itemize}
\item[] \emph{\textbf{Model D}}: Similar to Model C, but column-wise Softmax layer is no longer in the Pxl Header.
\end{itemize}

The structure of pixel classication branch of Model C and Model D is shown in Fig \ref{fig6-baselinePxlHeader}. Eventually, the training parameters of all models use same settings as described in Section \ref{subsubsec-trainingparams}

\begin{figure}[h]%
\centering
\includegraphics[width=0.55\linewidth]{fig6.pdf}
\caption{Pixel classification branch of baseline models (Model C and Model D)}\label{fig6-baselinePxlHeader}
\end{figure}

\FloatBarrier
\section{Results and Discussions}\label{sec-results}
Fig \ref{fig-sampledetection} presents some detection results that the original images are overlayed by boxes and polylines of groundtruths and predictions. 

\begin{figure*}[h]%
\centering
\includegraphics[width=\linewidth]{sample_detection.pdf}
\caption{Some detection samples from various models. A pair of two images presents a tapping panel image with the original size to scale (left) beside its magnified view (right). Detected or the predicted boxes with polyline insides are displayed in yellow. The dashed box with a polyline in red is the groundtruth}\label{fig-sampledetection}
\end{figure*}

The following results in this section are averaged from the models that tests with all testing data folds in K-fold cross-validaton fashion. Table \ref{tab-results-ap50} and Table \ref{tab-results-ap75} present average precisions of tapping line bounding box detection with 0.5 IoU ($\mathrm{AP}_{50}$) and 0.75 IoU ($\mathrm{AP}_{75}$) alongside distance errors of tapping endpoints, F1-scores, and the discrete Hausdorff distances (calculation manipulate the distance between polylines by distance between pixel vertices) that are averaged from the box-detected samples.

\begin{table*}[!h]
\centering
\caption{Detection results on the proposed model and baseline models at various settings. All metrics are averaged on the box-detected samples at 0.5 IoU}
\label{tab-results-ap50}
\begin{minipage}{\linewidth}
\begin{center}
\begin{tabular}{clccccc}
\toprule
Model & Pxl Header loss & \makecell[c]{Col-wise Softmax} & $\mathrm{AP_{75}}$ &$\mathrm{d_{ep}}$(px.) & $\mathrm{d_{H}}$(px.)& F1-score	\\ \midrule
A 	& Dice, Focal, Tversky 	& \checkmark & 0.98	&	12.08		& 	10.60	&	 0.5654 \\ \midrule
B 	& Dice, Focal, Tversky 	& - & 0.98	&	12.08		& 	10.60	&	 0.5654 \\ \midrule
C	& Dice		& \checkmark & 0.98	&	12.08	&	8.94	& 0.3511	\\
C	& Focal		& \checkmark & 0.96	&	12.68	&	12.31	&	0.3142	\\ 
C	& Tversky	& \checkmark& 0.98 	&	12.19	&  11.31	&	0.3510   \\ \midrule
D 	& Dice		& - & 0.98	&	12.60	&	11.18	&	0.5510	\\
D 	& Focal		& - & 0.95	&	13.22	&	13.54	&	0.3890	\\
D 	& Tversky	& - & 0.97	&	12.68	&	23.65	&	0.5602	\\ 
\bottomrule
\end{tabular}%
\end{center}
%\footnotetext{Note: Model code A, B, C refer to naming in Section \ref{sec-exp}}
\end{minipage}
\end{table*}

\begin{table*}[!h]
\centering
\caption{Detection results on the proposed model and baseline models at various settings. All metrics are averaged on the box-detected samples at 0.75 IoU}
\label{tab-results-ap75}
\begin{minipage}{\linewidth}
\begin{center}
\begin{tabular}{clccccc}
\toprule
Model & Pxl Header loss & \makecell[c]{Col-wise Softmax} & $\mathrm{AP_{75}}$ &$\mathrm{d_{ep}}$(px.) & $\mathrm{d_{H}}$(px.)& F1-score	\\ \midrule
A 	& Dice, Focal, Tversky 	& \checkmark & 0.62 & 9.14 	& 6.99 & 0.5831 \\ \midrule
B 	& Dice, Focal, Tversky 	& - & 0.62 & 9.14 	& 6.99 & 0.5831 \\ \midrule
C	& Dice		& \checkmark  & 0.60	&	8.90	&	6.60	& 0.3640	\\
C	& Focal		&\checkmark & 0.58 	& 9.25	&	9.34	&	0.3455	\\ 
C	& Tversky	& \checkmark &  0.61 		 & 9.15	&  8.69	&	0.3908   \\ \midrule
D 	& Dice		& - & 0.58 &	9.20	&	8.19	&	0.5744	\\
D 	& Focal		& - & 0.53 &	9.37	&	11.25	&	0.4028	\\
D 	& Tversky	& - & 0.58 & 9.13	&	22.06	&	0.5844 \\ 
\bottomrule
\end{tabular}%
\end{center}
%\footnotetext{Note: }
\end{minipage}
\end{table*}

Results in Table \ref{tab-results-ap50} and Table \ref{tab-results-ap75} clearly show that all models can detect tapping line boxes with high average precisions of 0.95 and aboves at 0.5 box IoU ($\mathrm{AP_{50}}$). Even in the more precise localization criterion, results with 0.75 box IoU ($\mathrm{AP_{75}}$) also confirm that all models also consistently produce tapping line bounding box at similar rates about 0.55 to 0.6 average precision. These IoU-based results are agreeable to tapping endpoints errors that measure the distance between boxes' corners. 

Difference between models are

\subsection{Overlapping and Distances}

\section{Conclusion}

Conclusions may be used to restate your hypothesis or research question, restate your major findings, explain the relevance and the added value of your work, highlight any limitations of your study, describe future directions for research and recommendations. 

In some disciplines use of Discussion or 'Conclusion' is interchangeable. It is not mandatory to use both. Please refer to Journal-level guidance for any specific requirements. 

\backmatter

\bmhead{Acknowledgments}

Acknowledgments are not compulsory. Where included they should be brief. Grant or contribution numbers may be acknowledged.

\section*{Declarations}

Some journals require declarations to be submitted in a standardised format. Please check the Instructions for Authors of the journal to which you are submitting to see if you need to complete this section. If yes, your manuscript must contain the following sections under the heading `Declarations':

\begin{itemize}
\item Availability of data and materials
\item Code availability 
\end{itemize}

\bibliographystyle{bst/sn-basic}
\bibliography{sn-bibliography}
\end{document}