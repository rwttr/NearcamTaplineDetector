\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{patent1,patent2,patent3,WANG2022103906,Zhou2021rubber}
\citation{Wongtanawijit_2021}
\citation{Wongtanawijit_2021}
\Newlabel{1}{1}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\newlabel{sec-introduction}{{1}{1}{Introduction}{section.1}{}}
\citation{unet}
\citation{vnet}
\citation{redmonyolov2}
\citation{Wongtanawijit_2021}
\citation{abraham1992tapping}
\citation{patent1,patent2,patent3}
\@writefile{toc}{\contentsline {section}{\numberline {2}Background and Motivation}{2}{section.2}\protected@file@percent }
\newlabel{sec-background}{{2}{2}{Background and Motivation}{section.2}{}}
\citation{Wongtanawijit_2021}
\citation{fasterrcnn}
\citation{redmonyolov1}
\citation{redmonyolov3}
\citation{alexyyolov4}
\citation{unet}
\citation{vnet}
\citation{segnet}
\citation{casenet}
\citation{maskrcnn}
\citation{vnet,gendice}
\citation{focalloss1}
\citation{tverskyloss}
\@writefile{toc}{\contentsline {section}{\numberline {3}Review of Literatures}{3}{section.3}\protected@file@percent }
\newlabel{sec-review}{{3}{3}{Review of Literatures}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Tapping Line Detection}{3}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Overview of Deep Convolutional Neural Networks}{3}{subsection.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Network Loss Function}{3}{subsection.3.3}\protected@file@percent }
\newlabel{review_loss}{{3.3}{3}{Network Loss Function}{subsection.3.3}{}}
\citation{focalloss1}
\citation{tverskyloss}
\citation{fastrcnn}
\citation{s20072145}
\citation{Lawal_2021,chentomato}
\citation{Li2021apple}
\citation{Chu2021apple,Jia2020apple}
\citation{Kuznetsova2020apple}
\citation{Yu2019strawberry}
\citation{Tong2020}
\citation{chentomato,Lawal_2021,Yu2019strawberry,Li2021apple}
\citation{Jia2020apple,Chu2021apple,Yu2019strawberry,Lawal2021tomato}
\citation{densenet}
\citation{resnet}
\citation{misra2019mish}
\citation{Li2021apple}
\citation{Chu2021apple}
\citation{zhang2019rubber,WANG2022103906}
\citation{Wongtanawijit_2021}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Deep Learning Computer Vision in Agricultural Robotics}{4}{subsection.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}Computer Vision for Rubber Latex Harvesting}{4}{subsection.3.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Methodology}{5}{section.4}\protected@file@percent }
\newlabel{sec-methodology}{{4}{5}{Methodology}{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Models}{5}{subsection.4.1}\protected@file@percent }
\newlabel{subsec-model}{{4.1}{5}{Models}{subsection.4.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.1}Downsampling Network}{5}{subsubsection.4.1.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Downsampling backbone network explained in block structures with given names (\textbf  {block}). The denoted labels are layer operation (ops), a 2-dimension convolution (Conv2d), a shortcut connection with addition (Residual), $s$: convolution stride, $n$: numbers of filters, filter kernel's size (\textbf  {filter}), and $c$ is the numbers of block replication.}}{5}{table.1}\protected@file@percent }
\newlabel{tab-darknetlight}{{1}{5}{Downsampling backbone network explained in block structures with given names (\textbf {block}). The denoted labels are layer operation (ops), a 2-dimension convolution (Conv2d), a shortcut connection with addition (Residual), $s$: convolution stride, $n$: numbers of filters, filter kernel's size (\textbf {filter}), and $c$ is the numbers of block replication}{table.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.2}Box Detection Branches}{5}{subsubsection.4.1.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The proposed tapping line detection network, visualized in a directed grpah with blocks. Network composes of four parts, from left-to-right are Downsampling network, two YOLO detection branches, Upsampling network, and Pixel classification branch.}}{6}{figure.1}\protected@file@percent }
\newlabel{fig1-modelA}{{1}{6}{The proposed tapping line detection network, visualized in a directed grpah with blocks. Network composes of four parts, from left-to-right are Downsampling network, two YOLO detection branches, Upsampling network, and Pixel classification branch}{figure.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Structure of a YOLO detection branch with the three prior boxes or anchors, where $n$ represents the number of filters, and activation function is denoted by $f(x)$. $w$ and $d$ orderly represents input tensor's spatial size and its depth, which may come from ``res4" or ``res5".}}{6}{table.2}\protected@file@percent }
\newlabel{tab-yolohead}{{2}{6}{Structure of a YOLO detection branch with the three prior boxes or anchors, where $n$ represents the number of filters, and activation function is denoted by $f(x)$. $w$ and $d$ orderly represents input tensor's spatial size and its depth, which may come from ``res4" or ``res5"}{table.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.3}Upsampling Path}{6}{subsubsection.4.1.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Configuration of U-Net-like Upsampling Path, where ops represent a transpose convolution path in Fig \ref  {fig1-modelA}, and $n$ is the number of filters}}{6}{table.3}\protected@file@percent }
\newlabel{tab-unetpath}{{3}{6}{Configuration of U-Net-like Upsampling Path, where ops represent a transpose convolution path in Fig \ref {fig1-modelA}, and $n$ is the number of filters}{table.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.4}Pixel Classification Branch}{6}{subsubsection.4.1.4}\protected@file@percent }
\citation{Wongtanawijit_2021}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Components inside a pixel prediction block (Pxl Header), the executions of ops perform in the order}}{7}{table.4}\protected@file@percent }
\newlabel{tab-pxlheader}{{4}{7}{Components inside a pixel prediction block (Pxl Header), the executions of ops perform in the order}{table.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Training}{7}{subsection.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.1}Dataset Priors and Statistics}{7}{subsubsection.4.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Visualization of estimated anchors. All box's aspect ratios are in scale. Dashed line represents the averaged box}}{7}{figure.2}\protected@file@percent }
\newlabel{fig2-anchors}{{2}{7}{Visualization of estimated anchors. All box's aspect ratios are in scale. Dashed line represents the averaged box}{figure.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.2}Branch Loss Function}{7}{subsubsection.4.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.3}Parameters Settings}{8}{subsubsection.4.2.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces WarmUp-Steady-Decay Learning Rate Profile}}{8}{figure.3}\protected@file@percent }
\newlabel{fig3-LR}{{3}{8}{WarmUp-Steady-Decay Learning Rate Profile}{figure.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.4}Regularization}{8}{subsubsection.4.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Evaluation Method}{8}{subsection.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Experiment Setup}{8}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Baseline Model and Variants}{8}{subsection.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Results and Discussions}{8}{section.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7}Conclusion}{8}{section.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Supplementary information}{8}{section*.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Acknowledgments}{8}{section*.3}\protected@file@percent }
\bibstyle{bst/sn-basic}
\bibdata{sn-bibliography}
\bibcite{abraham1992tapping}{{1}{1992}{{Abraham}}{{}}}
\bibcite{patent1}{{2}{2018{}}{{An~Feng}}{{}}}
\bibcite{patent2}{{3}{2018{}}{{An~Feng}}{{}}}
\bibcite{segnet}{{4}{2015}{{Badrinarayanan et~al}}{{Badrinarayanan, Kendall, and Cipolla}}}
\bibcite{alexyyolov4}{{5}{2020}{{Bochkovskiy et~al}}{{Bochkovskiy, Wang, and Liao}}}
\bibcite{chentomato}{{6}{2021}{{Chen et~al}}{{Chen, Wang, Wu, Hu, Zhao, Tan, Teng, and Luo}}}
\bibcite{Chu2021apple}{{7}{2021}{{Chu et~al}}{{Chu, Li, Lammers, Lu, and Liu}}}
\bibcite{fastrcnn}{{8}{2015}{{Girshick}}{{}}}
\bibcite{resnet}{{9}{2015}{{He et~al}}{{He, Zhang, Ren, and Sun}}}
\bibcite{maskrcnn}{{10}{2017}{{He et~al}}{{He, Gkioxari, Doll{\'{a}}r, and Girshick}}}
\bibcite{densenet}{{11}{2016}{{Huang et~al}}{{Huang, Liu, and Weinberger}}}
\bibcite{Jia2020apple}{{12}{2020}{{Jia et~al}}{{Jia, Tian, Luo, Zhang, Lian, and Zheng}}}
\bibcite{Kuznetsova2020apple}{{13}{2020}{{Kuznetsova et~al}}{{Kuznetsova, Maleva, and Soloviev}}}
\bibcite{Lawal_2021}{{14}{2021{}}{{Lawal}}{{}}}
\bibcite{Lawal2021tomato}{{15}{2021{}}{{Lawal}}{{}}}
\bibcite{Li2021apple}{{16}{2021}{{Li et~al}}{{Li, Jia, Sun, Hou, and Zheng}}}
\bibcite{focalloss1}{{17}{2017}{{Lin et~al}}{{Lin, Goyal, Girshick, He, and Doll{\'{a}}r}}}
\bibcite{s20072145}{{18}{2020}{{Liu et~al}}{{Liu, Nouaze, Touko~Mbouembe, and Kim}}}
\bibcite{vnet}{{19}{2016}{{Milletari et~al}}{{Milletari, Navab, and Ahmadi}}}
\bibcite{misra2019mish}{{20}{2019}{{Misra}}{{}}}
\bibcite{redmonyolov2}{{21}{2016}{{Redmon and Farhadi}}{{}}}
\bibcite{redmonyolov3}{{22}{2018}{{Redmon and Farhadi}}{{}}}
\bibcite{redmonyolov1}{{23}{2015}{{Redmon et~al}}{{Redmon, Divvala, Girshick, and Farhadi}}}
\bibcite{fasterrcnn}{{24}{2015}{{Ren et~al}}{{Ren, He, Girshick, and Sun}}}
\bibcite{unet}{{25}{2015}{{Ronneberger et~al}}{{Ronneberger, Fischer, and Brox}}}
\bibcite{tverskyloss}{{26}{2017}{{Salehi et~al}}{{Salehi, Erdogmus, and Gholipour}}}
\bibcite{gendice}{{27}{2017}{{Sudre et~al}}{{Sudre, Li, Vercauteren, Ourselin, and Cardoso}}}
\bibcite{Tong2020}{{28}{2020}{{Tong et~al}}{{Tong, Wu, and Zhou}}}
\bibcite{WANG2022103906}{{29}{2022}{{Wang et~al}}{{Wang, Zhou, Zhang, Ge, Li, Yuan, Zhang, and Zhang}}}
\bibcite{Wongtanawijit_2021}{{30}{2021}{{Wongtanawijit and Khaorapapong}}{{}}}
\bibcite{Yu2019strawberry}{{31}{2019}{{Yu et~al}}{{Yu, Zhang, Yang, and Zhang}}}
\bibcite{casenet}{{32}{2017}{{Yu et~al}}{{Yu, Feng, Liu, and Ramalingam}}}
\bibcite{zhang2019rubber}{{33}{2019}{{Zhang et~al}}{{Zhang, Yong, Chen, Zhang, Ge, Wang, and Li}}}
\bibcite{patent3}{{34}{2017}{{Zhang~Bin}}{{}}}
\bibcite{Zhou2021rubber}{{35}{2021}{{Zhou et~al}}{{Zhou, Zhang, Zhang, Zhang, Wang, Zhai, and Li}}}
\gdef \@abspage@last{10}
