\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{patent1,patent2,patent3,WANG2022103906,Zhou2021rubber}
\citation{Wongtanawijit_2021}
\citation{Wongtanawijit_2021}
\Newlabel{1}{1}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\newlabel{sec-introduction}{{1}{1}{Introduction}{section.1}{}}
\citation{unet}
\citation{vnet}
\citation{redmonyolov2}
\citation{Wongtanawijit_2021}
\citation{abraham1992tapping}
\citation{patent1,patent2,patent3}
\@writefile{toc}{\contentsline {section}{\numberline {2}Background and Motivation}{2}{section.2}\protected@file@percent }
\newlabel{sec-background}{{2}{2}{Background and Motivation}{section.2}{}}
\citation{Wongtanawijit_2021}
\citation{fasterrcnn}
\citation{redmonyolov1}
\citation{redmonyolov3}
\citation{alexyyolov4}
\citation{unet}
\citation{vnet}
\citation{segnet}
\citation{casenet}
\citation{maskrcnn}
\@writefile{toc}{\contentsline {section}{\numberline {3}Review of Literatures}{3}{section.3}\protected@file@percent }
\newlabel{sec-review}{{3}{3}{Review of Literatures}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Tapping Line Detection}{3}{subsection.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces A near-range tapping panel image (left) and the magnified cropped (right). The tapping line is annotated by a bounding box (dotted green rectangle) and a polyline ( multiple segments in red along the box's diagonal).}}{3}{figure.1}\protected@file@percent }
\newlabel{fig-sampleannotation}{{1}{3}{A near-range tapping panel image (left) and the magnified cropped (right). The tapping line is annotated by a bounding box (dotted green rectangle) and a polyline ( multiple segments in red along the box's diagonal)}{figure.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Overview of Deep Convolutional Neural Networks}{3}{subsection.3.2}\protected@file@percent }
\citation{vnet,gendice}
\citation{focalloss1}
\citation{tverskyloss}
\citation{focalloss1}
\citation{tverskyloss}
\citation{fastrcnn}
\citation{s20072145}
\citation{Lawal_2021,chentomato}
\citation{Li2021apple}
\citation{Chu2021apple,Jia2020apple}
\citation{Kuznetsova2020apple}
\citation{Yu2019strawberry}
\citation{Tong2020}
\citation{chentomato,Lawal_2021,Yu2019strawberry,Li2021apple}
\citation{Jia2020apple,Chu2021apple,Yu2019strawberry,Lawal2021tomato}
\citation{densenet}
\citation{resnet}
\citation{misra2019mish}
\citation{Li2021apple}
\citation{Chu2021apple}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Network Loss Function}{4}{subsection.3.3}\protected@file@percent }
\newlabel{review_loss}{{3.3}{4}{Network Loss Function}{subsection.3.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Deep Learning Computer Vision in Agricultural Robotics}{4}{subsection.3.4}\protected@file@percent }
\citation{zhang2019rubber,WANG2022103906}
\citation{Wongtanawijit_2021}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}Computer Vision for Rubber Latex Harvesting}{5}{subsection.3.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Methodology}{5}{section.4}\protected@file@percent }
\newlabel{sec-methodology}{{4}{5}{Methodology}{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Model}{5}{subsection.4.1}\protected@file@percent }
\newlabel{subsec-model}{{4.1}{5}{Model}{subsection.4.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Downsampling backbone network explained in block structures with given names (\textbf  {block}). The denoted labels are layer operation (ops), a 2-dimension convolution (Conv2d), a shortcut connection with addition (Residual), $s$: convolution stride, $n$: numbers of filters, filter kernel's size (\textbf  {filter}), and $c$ is the numbers of block replication.}}{5}{table.1}\protected@file@percent }
\newlabel{tab-darknetlight}{{1}{5}{Downsampling backbone network explained in block structures with given names (\textbf {block}). The denoted labels are layer operation (ops), a 2-dimension convolution (Conv2d), a shortcut connection with addition (Residual), $s$: convolution stride, $n$: numbers of filters, filter kernel's size (\textbf {filter}), and $c$ is the numbers of block replication}{table.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.1}Downsampling Network}{5}{subsubsection.4.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The proposed tapping line detection network, visualized in a directed grpah with blocks. Network composes of four parts, from left-to-right are Downsampling network, two YOLO detection branches, Upsampling network, and Pixel classification branch.}}{6}{figure.2}\protected@file@percent }
\newlabel{fig1-modelA}{{2}{6}{The proposed tapping line detection network, visualized in a directed grpah with blocks. Network composes of four parts, from left-to-right are Downsampling network, two YOLO detection branches, Upsampling network, and Pixel classification branch}{figure.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.2}Box Detection Branches}{6}{subsubsection.4.1.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Structure of a YOLO detection branch with the three prior boxes or anchors, where $n$ represents the number of filters, and activation function is denoted by $f(x)$. $w$ and $d$ orderly represents input tensor's spatial size and its depth, which may come from ``res4" or ``res5".}}{6}{table.2}\protected@file@percent }
\newlabel{tab-yolohead}{{2}{6}{Structure of a YOLO detection branch with the three prior boxes or anchors, where $n$ represents the number of filters, and activation function is denoted by $f(x)$. $w$ and $d$ orderly represents input tensor's spatial size and its depth, which may come from ``res4" or ``res5"}{table.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.3}Upsampling Path}{6}{subsubsection.4.1.3}\protected@file@percent }
\citation{Wongtanawijit_2021}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Configuration of U-Net-like Upsampling Path, where ops represent a transpose convolution path in Fig \ref  {fig1-modelA}, and $n$ is the number of filters}}{7}{table.3}\protected@file@percent }
\newlabel{tab-unetpath}{{3}{7}{Configuration of U-Net-like Upsampling Path, where ops represent a transpose convolution path in Fig \ref {fig1-modelA}, and $n$ is the number of filters}{table.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Components inside a pixel prediction block (Pxl Header), the executions of ops perform in the order}}{7}{table.4}\protected@file@percent }
\newlabel{tab-pxlheader}{{4}{7}{Components inside a pixel prediction block (Pxl Header), the executions of ops perform in the order}{table.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.4}Pixel Classification Branch}{7}{subsubsection.4.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Model Training}{7}{subsection.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.1}Dataset Priors and Statistics}{7}{subsubsection.4.2.1}\protected@file@percent }
\citation{tverskyloss}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Visualization of the estimated anchors. All box's aspect ratios are in scale. Dashed line represents the averaged box}}{8}{figure.3}\protected@file@percent }
\newlabel{fig2-anchors}{{3}{8}{Visualization of the estimated anchors. All box's aspect ratios are in scale. Dashed line represents the averaged box}{figure.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.2}Branch Loss Function}{8}{subsubsection.4.2.2}\protected@file@percent }
\newlabel{subsubsec-pxlbranch}{{4.2.2}{8}{Branch Loss Function}{subsubsection.4.2.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.3}Parameters Settings}{8}{subsubsection.4.2.3}\protected@file@percent }
\newlabel{subsubsec-trainingparams}{{4.2.3}{8}{Parameters Settings}{subsubsection.4.2.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces WarmUp-Steady-Decay Learning Rate Profile. Learning rate grows and drops in exponential rates in the warm-up and the decay periods.}}{8}{figure.4}\protected@file@percent }
\newlabel{fig3-LR}{{4}{8}{WarmUp-Steady-Decay Learning Rate Profile. Learning rate grows and drops in exponential rates in the warm-up and the decay periods}{figure.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.4}Generalization}{8}{subsubsection.4.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Model Inferencing}{8}{subsection.4.3}\protected@file@percent }
\citation{Wongtanawijit_2021}
\citation{Huttenlocher1993ComparingIU}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces A drawing shows a polyline and a box that are locating in a discrete unit grid. Assuming that a intersection on the grid represents a pixel location in an image. The red box represents the NMS-processed box with a plolyline (top). The result that the polyline are cropped and re-locate vectices to the box's corners is in bottom. }}{9}{figure.5}\protected@file@percent }
\newlabel{fig4-sampleagreement}{{5}{9}{A drawing shows a polyline and a box that are locating in a discrete unit grid. Assuming that a intersection on the grid represents a pixel location in an image. The red box represents the NMS-processed box with a plolyline (top). The result that the polyline are cropped and re-locate vectices to the box's corners is in bottom}{figure.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Evaluation Method}{9}{subsection.4.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces A drawing of two polylines in a discrete unit grid. Assuming that a intersection on the grid represents a pixel location in an image. A detected tapping line (dashed line) merely distances to the groundtruth (solid line) with 1 pixel is the exact overlapped.}}{9}{figure.6}\protected@file@percent }
\newlabel{fig4-polyline}{{6}{9}{A drawing of two polylines in a discrete unit grid. Assuming that a intersection on the grid represents a pixel location in an image. A detected tapping line (dashed line) merely distances to the groundtruth (solid line) with 1 pixel is the exact overlapped}{figure.6}{}}
\newlabel{eq-dice}{{1}{9}{Evaluation Method}{equation.4.1}{}}
\newlabel{eq-hausdorff}{{2}{9}{Evaluation Method}{equation.4.2}{}}
\newlabel{eq-endpoint}{{3}{10}{Evaluation Method}{equation.4.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Experiment Setup}{10}{section.5}\protected@file@percent }
\newlabel{sec-exp}{{5}{10}{Experiment Setup}{section.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Pixel classification branch of the proposed model}}{10}{figure.7}\protected@file@percent }
\newlabel{fig5-proposedPxlHeader}{{7}{10}{Pixel classification branch of the proposed model}{figure.7}{}}
\bibstyle{bst/sn-basic}
\bibdata{sn-bibliography}
\bibcite{abraham1992tapping}{{1}{1992}{{Abraham}}{{}}}
\bibcite{patent1}{{2}{2018{}}{{An~Feng}}{{}}}
\bibcite{patent2}{{3}{2018{}}{{An~Feng}}{{}}}
\bibcite{segnet}{{4}{2015}{{Badrinarayanan et~al}}{{Badrinarayanan, Kendall, and Cipolla}}}
\bibcite{alexyyolov4}{{5}{2020}{{Bochkovskiy et~al}}{{Bochkovskiy, Wang, and Liao}}}
\bibcite{chentomato}{{6}{2021}{{Chen et~al}}{{Chen, Wang, Wu, Hu, Zhao, Tan, Teng, and Luo}}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Pixel classification branch of baseline models (Model C and Model D)}}{11}{figure.8}\protected@file@percent }
\newlabel{fig6-baselinePxlHeader}{{8}{11}{Pixel classification branch of baseline models (Model C and Model D)}{figure.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Results and Discussions}{11}{section.6}\protected@file@percent }
\newlabel{sec-results}{{6}{11}{Results and Discussions}{section.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Overlapping and Distances}{11}{subsection.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7}Conclusion}{11}{section.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Acknowledgments}{11}{section*.2}\protected@file@percent }
\bibcite{Chu2021apple}{{7}{2021}{{Chu et~al}}{{Chu, Li, Lammers, Lu, and Liu}}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Some detection samples from various models. A pair of two images presents a tapping panel image with the original size to scale (left) beside its magnified view (right). Detected or the predicted boxes with polyline insides are displayed in yellow. The dashed box with a polyline in red is the groundtruth}}{12}{figure.9}\protected@file@percent }
\newlabel{fig-sampledetection}{{9}{12}{Some detection samples from various models. A pair of two images presents a tapping panel image with the original size to scale (left) beside its magnified view (right). Detected or the predicted boxes with polyline insides are displayed in yellow. The dashed box with a polyline in red is the groundtruth}{figure.9}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Detection results on the proposed model and baseline models at various settings. All metrics are averaged on the box-detected samples at 0.5 IoU}}{12}{table.5}\protected@file@percent }
\newlabel{tab-results-ap50}{{5}{12}{Detection results on the proposed model and baseline models at various settings. All metrics are averaged on the box-detected samples at 0.5 IoU}{table.5}{}}
\bibcite{fastrcnn}{{8}{2015}{{Girshick}}{{}}}
\bibcite{resnet}{{9}{2015}{{He et~al}}{{He, Zhang, Ren, and Sun}}}
\bibcite{maskrcnn}{{10}{2017}{{He et~al}}{{He, Gkioxari, Doll{\'{a}}r, and Girshick}}}
\bibcite{densenet}{{11}{2016}{{Huang et~al}}{{Huang, Liu, and Weinberger}}}
\bibcite{Huttenlocher1993ComparingIU}{{12}{1993}{{Huttenlocher et~al}}{{Huttenlocher, Klanderman, and Rucklidge}}}
\bibcite{Jia2020apple}{{13}{2020}{{Jia et~al}}{{Jia, Tian, Luo, Zhang, Lian, and Zheng}}}
\bibcite{Kuznetsova2020apple}{{14}{2020}{{Kuznetsova et~al}}{{Kuznetsova, Maleva, and Soloviev}}}
\bibcite{Lawal_2021}{{15}{2021{}}{{Lawal}}{{}}}
\bibcite{Lawal2021tomato}{{16}{2021{}}{{Lawal}}{{}}}
\bibcite{Li2021apple}{{17}{2021}{{Li et~al}}{{Li, Jia, Sun, Hou, and Zheng}}}
\bibcite{focalloss1}{{18}{2017}{{Lin et~al}}{{Lin, Goyal, Girshick, He, and Doll{\'{a}}r}}}
\bibcite{s20072145}{{19}{2020}{{Liu et~al}}{{Liu, Nouaze, Touko~Mbouembe, and Kim}}}
\bibcite{vnet}{{20}{2016}{{Milletari et~al}}{{Milletari, Navab, and Ahmadi}}}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces Detection results on the proposed model and baseline models at various settings. All metrics are averaged on the box-detected samples at 0.75 IoU}}{13}{table.6}\protected@file@percent }
\newlabel{tab-results-ap75}{{6}{13}{Detection results on the proposed model and baseline models at various settings. All metrics are averaged on the box-detected samples at 0.75 IoU}{table.6}{}}
\bibcite{misra2019mish}{{21}{2019}{{Misra}}{{}}}
\bibcite{redmonyolov2}{{22}{2016}{{Redmon and Farhadi}}{{}}}
\bibcite{redmonyolov3}{{23}{2018}{{Redmon and Farhadi}}{{}}}
\bibcite{redmonyolov1}{{24}{2015}{{Redmon et~al}}{{Redmon, Divvala, Girshick, and Farhadi}}}
\bibcite{fasterrcnn}{{25}{2015}{{Ren et~al}}{{Ren, He, Girshick, and Sun}}}
\bibcite{unet}{{26}{2015}{{Ronneberger et~al}}{{Ronneberger, Fischer, and Brox}}}
\bibcite{tverskyloss}{{27}{2017}{{Salehi et~al}}{{Salehi, Erdogmus, and Gholipour}}}
\bibcite{gendice}{{28}{2017}{{Sudre et~al}}{{Sudre, Li, Vercauteren, Ourselin, and Cardoso}}}
\bibcite{Tong2020}{{29}{2020}{{Tong et~al}}{{Tong, Wu, and Zhou}}}
\bibcite{WANG2022103906}{{30}{2022}{{Wang et~al}}{{Wang, Zhou, Zhang, Ge, Li, Yuan, Zhang, and Zhang}}}
\bibcite{Wongtanawijit_2021}{{31}{2021}{{Wongtanawijit and Khaorapapong}}{{}}}
\bibcite{Yu2019strawberry}{{32}{2019}{{Yu et~al}}{{Yu, Zhang, Yang, and Zhang}}}
\bibcite{casenet}{{33}{2017}{{Yu et~al}}{{Yu, Feng, Liu, and Ramalingam}}}
\bibcite{zhang2019rubber}{{34}{2019}{{Zhang et~al}}{{Zhang, Yong, Chen, Zhang, Ge, Wang, and Li}}}
\bibcite{patent3}{{35}{2017}{{Zhang~Bin}}{{}}}
\bibcite{Zhou2021rubber}{{36}{2021}{{Zhou et~al}}{{Zhou, Zhang, Zhang, Zhang, Wang, Zhai, and Li}}}
\gdef \@abspage@last{14}
